{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ðŸŽ›ï¸Program Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:59:13.780351Z",
     "start_time": "2023-09-11T11:59:13.515559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard Python Imports\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# Third-Party Imports (requires pip install)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local Libraries\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # We want to visualise all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Don't limit the width of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T12:10:26.904212Z",
     "start_time": "2023-09-11T12:10:26.652103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Case 2 Globals\n",
    "DATA_PATH = './jiraEvolutions/'\n",
    "\n",
    "LOG = utils.CustomLogger('CustomLogger', log_level='info', display_loglevel=False, display_datetime=False)\n",
    "PICKLE_LIB = utils.PickleLib(data_path=DATA_PATH, logger=LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T11:39:24.805829Z",
     "start_time": "2023-09-11T11:39:24.802247Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the evolution dataframe from the GenerateEvolutionDataframe script\n",
    "evo_df = PICKLE_LIB.pickle_load(f\"{DATA_PATH}load_evolution_dataframe(jiras=[_Hyperledger_])\", 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to each for the combind \"Jira Issue ID\", which is a combination of the Jira name and the Issue ID.\n",
    "# This field creates a truly unique ID across Jiras and issues.\n",
    "evo_df['jira_issue_id'] = evo_df.jira + ' ' + evo_df.issue_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data to just be description changes\n",
    "# evo_df = evo_df[evo_df.field == 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a minimum required number of evolutions for our analysis, where the creation itself counts as an evolution.\n",
    "# minimum_evolutions = 2\n",
    "# evo_df = evo_df.groupby('jira_issue_id').filter(lambda x: len(x.index) >= minimum_evolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NLP Techniques for Identifying Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Collect Target Entities: Fields and Field States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in identifying entities that match issue fields. We want both the field names themselves, as well as\n",
    "the possible states of those fields. To begin, we will create a complete list of all field names, and every value that\n",
    "has ever been set to those fields, organised per field. We will segment this analysis on a per-Jira level, but you could\n",
    "choose any segmentation (or not) of the data based on your level of analysis. Since fields and their available options\n",
    "are set on a per-Jira level, this is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fields_and_states():\n",
    "\n",
    "    # Collect all unique field states\n",
    "    field_states = defaultdict(dict)\n",
    "    \n",
    "    # First, collect a list of all fields in our dataset. This is just the list of unique values in our \"fields\" column\n",
    "    all_fields = list(evo_df.field.unique())\n",
    "\n",
    "    # We don't want to extract the states for certain fields, such as the Summary and Description\n",
    "    fields_to_ignore_state = ['Summary', 'Description', 'Comments', 'CreatedDate', 'ResolvedDate']\n",
    "    fields_to_extract_states = [f for f in all_fields if f not in fields_to_ignore_state]\n",
    "\n",
    "    # The analysis is per-Jira, so we need a list of all Jiras\n",
    "    all_jiras = list(evo_df.jira.unique())\n",
    "\n",
    "    # For each field, get all unique states this field has ever been in\n",
    "    for field in fields_to_extract_states:\n",
    "\n",
    "        # Reduce the dataset to just the relevant field entries\n",
    "        evo_field_df = evo_df[evo_df.field == field]\n",
    "\n",
    "        # Segment the unique field states that are used within each Jira\n",
    "        for jira in all_jiras:\n",
    "\n",
    "            # Reduce the dataset to just the relevant jira entries\n",
    "            evo_jira_field_df = evo_field_df[evo_field_df.jira == jira]\n",
    "\n",
    "            # Get all unqiue states this field has ever been in: stored in the data_from and data_to columns\n",
    "            all_states = set(list(evo_jira_field_df.data_from) + list(evo_jira_field_df.data_to))\n",
    "\n",
    "            # Convert all states to a string. This is required for our comparison to strings later\n",
    "            all_states = set([str(state) for state in all_states if str(state).strip()])\n",
    "\n",
    "            # Save all field jira states\n",
    "            field_states[field][jira] = all_states\n",
    "\n",
    "        # Now that we have gathered all unique states for this field across all Jiras, we want to create two more sets\n",
    "        # per field: all_jiras_intersection and all_jiras_union. This allows us to check some other interesting things.\n",
    "        field_states[field]['all_jiras_intersection'] = set.intersection(*list(field_states[field].values()))\n",
    "        field_states[field]['all_jiras_union'] = set.union(*list(field_states[field].values()))\n",
    "\n",
    "    return utils.defaultdict_to_dict(field_states)\n",
    "\n",
    "field_states = collect_fields_and_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the fields and field state counts. There are too many field states to reasonably visualise them.\n",
    "def display_field_states_counts():\n",
    "\n",
    "    # First, create a dict of dicts, where each dict represents a single field, and each item is the count within a Jira\n",
    "    field_states_counts = {}\n",
    "    for field, field_obj in field_states.items():\n",
    "        field_states_counts[field] = {}\n",
    "        for jira, jira_field_obj in field_obj.items():\n",
    "            field_states_counts[field][jira] = len(jira_field_obj)\n",
    "    \n",
    "    # Convert dict of dicts into a dataframe, and display it\n",
    "    display(pd.DataFrame(field_states_counts))\n",
    "\n",
    "display_field_states_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(field_states['IssueType']['all_jiras_intersection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(field_states['Priority']['all_jiras_intersection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(field_states['Status']['all_jiras_intersection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(field_states['Resolution']['all_jiras_intersection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Target Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Method: Text must contain 1) a field name and 2) any field value we found earlier (for that field and Jira)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discussion_analysis_items(evo_df, field_states, fields_to_analyse=None, sample_issue_num=None):\n",
    "\n",
    "    def save_discussion_item(jira_issue_id, text, field, field_state):\n",
    "        issue_discussion_items[jira_issue_id][field].append({\n",
    "            'field_state': field_state,\n",
    "            'text': text,\n",
    "        })\n",
    "\n",
    "    # If not specified, analyse all fields in evo_df\n",
    "    if not fields_to_analyse:\n",
    "        fields_to_analyse = list(evo_df.field.unique())\n",
    "\n",
    "    # We are only analysing the Description and Comments\n",
    "    # evo_df = evo_df[evo_df.field.isin(['Description', 'Comments'])]\n",
    "    evo_df = evo_df[evo_df.field.isin(['Comments'])]\n",
    "    # We are not interested in analysing the \"creational\" evolutions\n",
    "    evo_df = evo_df[evo_df.history_order > 0]\n",
    "\n",
    "    # Get the set of unique issue ids in our evolution dataframe\n",
    "    jira_issue_ids = list(np.unique(evo_df.jira_issue_id))\n",
    "    # Shuffle the data, so people running \"sample_issue_num\" get different ones each time\n",
    "    random.shuffle(jira_issue_ids)\n",
    "    \n",
    "    # Store all identified discussion items\n",
    "    issue_discussion_items = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # For each issue, check the Description and Comments for the target fields\n",
    "    # for jira_issue_id in jira_issue_ids:\n",
    "    for jira_issue_id in tqdm(jira_issue_ids, total=len(jira_issue_ids), ncols=100, ascii=True):\n",
    "\n",
    "        # Reduce evo_df to just the relevant data\n",
    "        evo_jira_issue_df = evo_df[evo_df.jira_issue_id == jira_issue_id]\n",
    "\n",
    "        # Extract the jira of this issue, for future use\n",
    "        issue_jira = evo_jira_issue_df.iloc[0].jira\n",
    "\n",
    "        # Analyse every \"data_to\" text field\n",
    "        for _, evolution in evo_jira_issue_df.iterrows():\n",
    "\n",
    "            # Extract the text\n",
    "            text = evolution.data_to\n",
    "            if not isinstance(text, str):\n",
    "                continue  # The text must be a string\n",
    "\n",
    "            # Check all requested fields\n",
    "            for field in fields_to_analyse:\n",
    "\n",
    "                if field not in text:\n",
    "                    continue  # We didn't find any mention of this field\n",
    "                \n",
    "                # We want to check every past field state\n",
    "                for field_state in field_states[field][issue_jira]:\n",
    "                    if field_state in text:\n",
    "                        # Save this item\n",
    "                        save_discussion_item(jira_issue_id, text, field, field_state)\n",
    "        \n",
    "        # Check if we have enough items based on the requested sample\n",
    "        if sample_issue_num and len(issue_discussion_items) == sample_issue_num:\n",
    "            break\n",
    "    \n",
    "    return utils.defaultdict_to_dict(issue_discussion_items)\n",
    "\n",
    "issue_discussion_items = get_discussion_analysis_items(\n",
    "    evo_df, field_states, fields_to_analyse=['Resolution'], sample_issue_num=None)\n",
    "\n",
    "\n",
    "# IssueType Priority Status Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(issue_discussion_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of issues with Discussion Items: {len(issue_discussion_items)}\")\n",
    "print(f\"Number of total Discussion Items: {sum([len(n_item) for item in issue_discussion_items.values() for n_item in item.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_issue_discussion_items(issue_discussion_items, sample_issue_num=None):\n",
    "\n",
    "    # For time reasons, it may be better to just display a sample of issues\n",
    "    if sample_issue_num and sample_issue_num < len(issue_discussion_items):\n",
    "        # Get a random sample of keys\n",
    "        issues_ambs_found_keys = random.sample(list(issue_discussion_items.keys()), sample_issue_num)\n",
    "        # Use those keys to build a sample of the original dict\n",
    "        issue_discussion_items = {key: issue_discussion_items[key] for key in issues_ambs_found_keys}\n",
    "    \n",
    "    # Display each group of ambiguities, one issue at a time\n",
    "    for jira_issue_id, issue_discussion_items in issue_discussion_items.items():\n",
    "        for field, issue_field_discussion_items in issue_discussion_items.items():\n",
    "            # Print the identifiers of this discussion item\n",
    "            print(f\"\\n{'-'*50} New Discussion Item {'-'*50}\\n\")\n",
    "            print(f\"{jira_issue_id} {field}\")\n",
    "            pprint(issue_field_discussion_items)\n",
    "\n",
    "\n",
    "# Display the discussion items found, one issue at a time\n",
    "display_issue_discussion_items(issue_discussion_items, sample_issue_num=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "item = \"issueType\"\n",
    "\n",
    "# Save the issue_discussion_items to a txt file\n",
    "with open(\"./issueDiscussionItems/\"+ item +\"(JiraEcosystemEvoDF).txt\", \"w\") as file:\n",
    "    file.write(json.dumps(issue_discussion_items, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
