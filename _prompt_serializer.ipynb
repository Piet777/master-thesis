{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take a look at this guides:\n",
    "- (Korzynski, Mazurek, Krzypkowska, & Kurasinski, 2023)\n",
    "\n",
    "Use Template pattern for output format promtp\n",
    "- (White, Fu, Hays, Sandborn, Olea, Gilbert, Elnashar, Spencer-Smith, & Schmidt, 2023)\n",
    "\n",
    "Use PB Smells form Qamar et Al 2021 (see overleaf chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Versioning: https://semverdoc.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_temp = \"\"\"### Instruction ###\n",
    "Act as a professional {role} working with issue trackers like Jira or Azure DevOps.\n",
    "Working with these issue trackers, you must consider multiple text-based best practices to increase understandability and reduce time.\n",
    "Your outputs should correspond to those a {role} would regarding the ticket.\n",
    "We want to revise a ticket for compliance with these best practices and offer recommendations if needed.\n",
    "\n",
    "### Context ###\n",
    "A ticket consists of multiple fields. You are provided with the fields and a short description for each field.\n",
    "Assignee: The person responsible to resolve the issue.\n",
    "Comments: Community discussion on the issue, including author, timestamp, and content.\n",
    "Components: Project components to which the issue belongs.\n",
    "CreatedDate: The time and date the issue was created.\n",
    "Creator: The person who created the issue.\n",
    "Description: A detailed description of the issue.\n",
    "IssueLinks: A list of links to related issues.\n",
    "IssueType: The issue's purpose within the organization.\n",
    "Labels: Labels to which this issue relates.\n",
    "Priority: The issue importance in relation to other issues.\n",
    "Project: The parent project to which the issue belongs.\n",
    "Reporter: The person who found/reported the issue.\n",
    "Resolution: A record of the issue's resolution, once resolved or closed.\n",
    "ResolvedDate: The time and date the issue was resolved.\n",
    "Status: The stage the issue is currently at in its lifecycle.\n",
    "Summary: A brief one-line summary of the issue.\n",
    "TimeSpent: Amount of time spent working on the issue.\n",
    "VersionsAffected: The versions of the project affected by the issue.\n",
    "VersionsFixed: Project versions in which the issue was (or will be) fixed.\n",
    "\n",
    "### Best Practice ###\n",
    "The best practice to check is {best_practice}\"\"\"\n",
    "init_prompt = PromptTemplate(template= init_temp, input_variables= [\"role\", \"best_practice\"])\n",
    "\n",
    "init_prompt.save(\"prompts/init/initPrompt_V2.1.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Prompt (Best Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_temp =\"\"\"Best Practice: Update Missing/Outdated Fields\n",
    "During the evolution of a ticket, some fields may have become outdated or missing.\n",
    "You could find the needed information in the \"Summary\", \"Description\", or \"Comments\" fields.\n",
    "Get contextual knowledge about the ticket through the \"Summary\", \"Description\", and \"Comments\" fields.\n",
    "If someone specifically mentions or suggests a change for the ticket in the \"Comments\" field, update the ticket accordingly.\n",
    "The change can be provided by a human or as a message from a programm.\n",
    "Ignore error messages, logs and quotes within the comment.\n",
    "Possible keywords for such a change are: \"edit\", \"change\", \"move\", \"mark\", \"fix\", \"set\", \"update\", \"modify\", \"correct\", \"adjust\", \"resolve\" and \"should\".\n",
    "Pay particular attention to the most recent comments.\n",
    "\n",
    "Return only mentioned or suggested changes from the \"Comments\" field.\"\"\"\n",
    "\n",
    "update_prompt = PromptTemplate(template= update_temp, input_variables= [])\n",
    "update_prompt.save(\"prompts/update/updatePrompt_V1.26.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output format prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_format_temp =\"\"\"### Instruction ###\n",
    "You receive changes and recommendations made to fields of a ticket from an issue tracker.\n",
    "Return the data  in the following format as JSON-format:\n",
    "\"<violation_predicted>\": \"<Set to TRUE, if a change was detected and made, otherwise FALSE>\",\n",
    "\"fields\": \"<Contains the list for the receives changes, icnluding <field>, <recommendation>, and <reason>>\"\n",
    "\"field\": \"<name of updated field>\", \n",
    "\"changed_to\": \"<the new input of the field after update, without any explanation>\", \n",
    "\"reason\": \"<reason for the update>\"\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_ticket}\"\"\"\n",
    "\n",
    "update_format_prompt = PromptTemplate(template= update_format_temp, input_variables= [\"revised_ticket\"])\n",
    "\n",
    "update_format_prompt.save(\"prompts/update/updateFormatPrompt_V1.12.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Length Prompt (Best Practice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examples:\n",
    "- https://bugzilla.mozilla.org/page.cgi?id=bug-writing.html\n",
    "- https://bugs.eclipse.org/bugs/page.cgi?id=bug-writing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_temp = \"\"\"'Summary Length':\n",
    "Your Task is to check if the ticket summary has exactly between {min} and {max} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {ticket}\"\"\"\n",
    "\n",
    "sumLen_prompt = PromptTemplate(template= sumLen_temp, input_variables= [\"min\", \"max\", \"ticket\"])\n",
    "\n",
    "sumLen_prompt.save(\"prompts/summary/summaryLengthPrompt_V3.2.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot CoT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_0ShotCoT_temp = \"\"\"Best Practice: Summary Length\n",
    "Your Task is to check if the ticket summary has exactly between {min} and {max} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {ticket}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "sumLen_0ShotCoT_prompt = PromptTemplate(template= sumLen_0ShotCoT_temp, input_variables= [\"min\", \"max\", \"ticket\"])\n",
    "\n",
    "sumLen_0ShotCoT_prompt.save(\"prompts/summary/summaryLengthPrompt_0ShotCoT_V2.1.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Sourcetree crashes',\n",
    "'Description':'Source tree crashes everytime i open , I have uninstalled and installed many times',\n",
    "'VersionsAffected':'3.4.4',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Sourcetree for Windows',\n",
    "'Components':'Git'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Sourcetree crashes\", which consists of 18 characters. This is below the recommended range of 39 to 70 characters for a summary.\n",
    "\n",
    "Recommended Summary: \"Sourcetree crashes upon opening repeatedly\"\n",
    "\n",
    "This revised summary is 42 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Opening and closing Sourcetree causes Pageant to crash\\/close',\n",
    "'Description':'I have installed putty and pageant outside of Sourcetree, and I have Pageant startup during OS boot so my key-file is always ready.\\r\\n\\r\\nDidn't quite notice it at first, so not sure which version it started at, but Sourcetree now causes Pageant to crash\\/close when it opens or closes, causing e.g. git commands and such from command line to suddenly start to failing for no apparent reason.\\r\\n\\r\\nWhenever I open or close Sourcetree now, I have to reopen Pageant and reload my key, including entering a long passkey. Super annoying. Sourcetree shouldn't close or cause any other applications to crash when it opens or when it closes.\\r\\n\\r\\nIt happens on all 3 computers of mine. All running latest Windows, latest Pageant, and latest Sourcetree.',\n",
    "'VersionsAffected':'3.4.4',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Sourcetree for Windows',\n",
    "'Components':'General'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Opening and closing Sourcetree causes Pageant to crash\\/close\" which consists of 61 characters. This is within the best practice range of 39 to 70 characters.\n",
    "\n",
    "No summary has to be recommended. \n",
    "\n",
    "The original summary is compliant with the best practice for summary length and does not require any changes.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Cannot change title of News tool',\n",
    "'Description':'After initially setting up the News tool during worksite setup, I tried to go to the option page (NEWS-2) and change the title of the tool. After I made the change, the title of the tool did not update, even after I logged out and logged back in. However, when I opened up options, the new title that I had entered was still specified.\\n\\nI tried to change the URL of the feed as well as the title at the same time, and even though the new feed was displayed properly, the title of the tool remained what I had initially set it to during worksite setup.',\n",
    "'VersionsAffected':'2.0',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Sakai',\n",
    "'Components':'News (RSS)'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Cannot change title of News tool\", which consists of 32 characters. This is below the recommended range of 39 to 70 characters for a summary.\n",
    "\n",
    "Recommended Summary: \"Title of News tool remains unchanged after several update attempts\"\n",
    "\n",
    "This revised summary is 66 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Game Crashes When Player Opens Inventory While Respawning with 'Immediate Respawn' Enabled',\n",
    "'Description':'If the player opens their inventory quickly after respawning in a world with 'Immediate Respawn' enabled, the world will crash. This happened accidentally once in one of my worlds. I tried it a few more times after reloading, and it seemed to happen reliably as long as I opened my inventory at the right time.',\n",
    "'VersionsAffected':'PS4 1.89',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Minecraft Console Edition',\n",
    "'Components':null}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Game Crashes When Player Opens Inventory While Respawning with 'Immediate Respawn' Enabled\", which consists of 90 characters. This exceeds the recommended range of 39 to 70 characters for a summary.\n",
    "\n",
    "Recommended Summary: \"Crash on inventory access with active 'Immediate Respawn' on respawn\"\n",
    "\n",
    "This revised summary is 68 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Epic link gets removed when moving an epic from a non next gen project to a next project',\n",
    "'Description':'h3. Summary \\r\\n\\r\\n When an epic from a non next-Gen project is moved to a next-Gen project, the association is removed. All issues associated with the epics will  no longer be linked to the epic \\r\\n\\r\\n h3. Steps to reproduce \\r\\n\\r\\n # Have 2 projects (Next-gen and non next-gen) \\r\\n # Create an Epic from a non Next-gen project \\r\\n # Add a child issue to the epic \\r\\n # Move the epic to the next-gen project \\r\\n\\r\\n h3. Expected Result \\r\\n Epic link remains. The issues are still linked with the epic in the next-gen project even after the moved. \\r\\n\\r\\n h3. Actual Result \\r\\n Epic link is lost',\n",
    "'VersionsAffected':null,\n",
    "'IssueType':'Bug',\n",
    "'Project':'Jira Software Cloud',\n",
    "'Components':'next-gen'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Epic link gets removed when moving an epic from a non next gen project to a next project\", which consists of 88 characters. This exceeds the recommended range of 39 to 70 characters for a summary.\n",
    "\n",
    "Recommended Summary: \"Epic link loss on moving an epic to a 'next-gen' project\"\n",
    "\n",
    "This revised summary is 56 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_FewShot_temp = PromptTemplate(\n",
    "    input_variables=[\"minChar\", \"maxChar\", \"input\", \"output\"], \n",
    "    template=\"\"\"'Summary Length':\n",
    "Your Task is to check if the ticket summary has exactly between {minChar} and {maxChar} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {input}\n",
    "Answer: {output}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_FewShot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=sumLen_FewShot_temp,\n",
    "    suffix= \"\"\"'Summary Length':\n",
    "Your Task is to check if the ticket summary has exactly between {min} and {max} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {ticket}\n",
    "Answer: \"\"\",\n",
    "    input_variables=[\"min\", \"max\", \"ticket\"],\n",
    ")\n",
    "\n",
    "sumLen_FewShot_prompt.save(\"prompts/summary/summaryLengthPrompt_FewShot_V2.3.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot CoT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Sourcetree crashes',\n",
    "'Description':'Source tree crashes everytime i open , I have uninstalled and installed many times',\n",
    "'VersionsAffected':'3.4.4',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Sourcetree for Windows',\n",
    "'Components':'Git'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Sourcetree crashes\". Splitting the summary into it's individual characters, we get the following list: ['S', 'o', 'u', 'r', 'c', 'e', 't', 'r', 'e', 'e', ' ', 'c', 'r', 'a', 's', 'h', 'e', 's'].\n",
    "Counting each character including all letters, spaces, and punctuation marks, we get a total of 18 characters. This is below the recommended range of 39 to 70 characters for a summary, so a new summary has to be recommended.\n",
    "\n",
    "Recommended Summary: \"Sourcetree crashes upon opening repeatedly\"\n",
    "\n",
    "This revised summary is 42 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"minChar\": 39,\n",
    "        \"maxChar\": 70,\n",
    "        \"input\": \"\"\"{{'Summary':'Opening and closing Sourcetree causes Pageant to crash\\/close',\n",
    "'Description':'I have installed putty and pageant outside of Sourcetree, and I have Pageant startup during OS boot so my key-file is always ready. Didn't quite notice it at first, so not sure which version it started at, but Sourcetree now causes Pageant to crash\\/close when it opens or closes, causing e.g. git commands and such from command line to suddenly start to failing for no apparent reason. Whenever I open or close Sourcetree now, I have to reopen Pageant and reload my key, including entering a long passkey. Super annoying. Sourcetree shouldn't close or cause any other applications to crash when it opens or when it closes. It happens on all 3 computers of mine. All running latest Windows, latest Pageant, and latest Sourcetree.',\n",
    "'VersionsAffected':'3.4.4',\n",
    "'IssueType':'Bug',\n",
    "'Project':'Sourcetree for Windows',\n",
    "'Components':'General'}}\"\"\",\n",
    "        \"output\": \"\"\"The current summary is \"Opening and closing Sourcetree causes Pageant to crash\\/close\". Splitting the summary into it's individual characters, we get the following list: ['O', 'p', 'e', 'n', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'o', 's', 'i', 'n', 'g', ' ', 'S', 'o', 'u', 'r', 'c', 'e', 't', 'r', 'e', 'e', ' ', 'c', 'a', 'u', 's', 'e', 's', ' ', 'P', 'a', 'g', 'e', 'a', 'n', 't', ' ', 't', 'o', ' ', 'c', 'r', 'a', 's', 'h', '\\', '/', 'c', 'l', 'o', 's', 'e'].\n",
    "Counting each character including all letters, spaces, and punctuation marks, we get a total of 61 characters. This is within the best practice range of 39 to 70 characters.    \n",
    "        \n",
    "No summary has to be recommended. \n",
    "\n",
    "The original summary is compliant with the best practice for summary length and does not require any changes.\"\"\"\n",
    "     },\n",
    "#     {\n",
    "#         \"minChar\": 39,\n",
    "#         \"maxChar\": 70,\n",
    "#         \"input\": \"\"\"{{'Summary':'Game Crashes When Player Opens Inventory While Respawning with 'Immediate Respawn' Enabled',\n",
    "# 'Description':'If the player opens their inventory quickly after respawning in a world with 'Immediate Respawn' enabled, the world will crash. This happened accidentally once in one of my worlds. I tried it a few more times after reloading, and it seemed to happen reliably as long as I opened my inventory at the right time.',\n",
    "# 'VersionsAffected':'PS4 1.89',\n",
    "# 'IssueType':'Bug',\n",
    "# 'Project':'Minecraft Console Edition',\n",
    "# 'Components':null}}\"\"\",\n",
    "#         \"output\": \"\"\"The current summary is \"Game Crashes When Player Opens Inventory While Respawning with 'Immediate Respawn' Enabled\". Splitting the summary into it's individual characters, we get the following list: ['G', 'a', 'm', 'e', ' ', 'C', 'r', 'a', 's', 'h', 'e', 's', ' ', 'W', 'h', 'e', 'n', ' ', 'P', 'l', 'a', 'y', 'e', 'r', ' ', 'O', 'p', 'e', 'n', 's', ' ', 'I', 'n', 'v', 'e', 'n', 't', 'o', 'r', 'y', ' ', 'W', 'h', 'i', 'l', 'e', ' ', 'R', 'e', 's', 'p', 'a', 'w', 'n', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', '\\'', 'I', 'm', 'm', 'e', 'd', 'i', 'a', 't', 'e', ' ', 'R', 'e', 's', 'p', 'a', 'w', 'n', '\\'', ' ', 'E', 'n', 'a', 'b', 'l', 'e', 'd'].\n",
    "# Counting each character including all letters, spaces, and punctuation marks, we get a total of 90 characters. This exceeds the recommended range of 39 to 70 characters for a summary, so a new summary has to be recommended.\n",
    "\n",
    "# Recommended Summary: \"Crash on inventory access with active 'Immediate Respawn' on respawn\"\n",
    "\n",
    "# This revised summary is 68 characters long, fitting within the recommended range and succinctly conveying the essence of the ticket.\"\"\"\n",
    "#     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_CoT_temp = PromptTemplate(\n",
    "    input_variables=[\"minChar\", \"maxChar\", \"input\", \"output\"], \n",
    "    template=\"\"\"'Summary Length':\n",
    "Your Task is to check if the ticket summary has exactly between {minChar} and {maxChar} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {input}\n",
    "Answer: Let's think step by step. {output}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_CoT_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=sumLen_CoT_temp,\n",
    "    suffix= \"\"\"'Summary Length':\n",
    "Your Task is to check if the ticket summary has exactly between {min} and {max} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "The ticket is in JSON format.\n",
    "Ticket: {ticket}\n",
    "Answer: Let's think step by step.\"\"\",\n",
    "    input_variables=[\"min\", \"max\", \"ticket\"],\n",
    ")\n",
    "\n",
    "sumLen_CoT_prompt.save(\"prompts/summary/summaryLengthPrompt_CoT_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Format Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_format_temp = \"\"\"### Instruction ###\n",
    "You receive a revised ticket from an issue tracker in JSON format.\n",
    "Return the given Data in the following format in JSON:\n",
    "\"<violation_predicted>\": \"<Set to TRUE, if a change was detected and made, otherwise FALSE>\"\n",
    "\"<summary_old>\": \"<The current and unchanged content of the summary field>\"\n",
    "\"<summary_new>\": \"<The recommended and revised content of the summary field>\"\n",
    "\n",
    "If the summary was not changed, return the original and unchanged summary in the \"<summary_new>\" field.\n",
    "Don't return any source code or code snippets.\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_ticket}\"\"\"\n",
    "\n",
    "sumLen_format_prompt = PromptTemplate(template= sumLen_format_temp, input_variables= [\"revised_ticket\"])\n",
    "\n",
    "sumLen_format_prompt.save(\"prompts/summary/summaryLengthFormatPrompt_V1.5.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consensus Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_consensus_temp = \"\"\"### Instruction ###\n",
    "Act as a professional {role} working with issue trackers like Jira or Azure DevOps.\n",
    "Working with these issue trackers, you must consider multiple text-based best practices to increase understandability and reduce time.\n",
    "Your outputs should correspond to those a {role} would regarding the ticket.\n",
    "\n",
    "Previously, several prompt algorithms were tested for the best practice 'Summary Length'. This best practice states:\n",
    "Your Task is to check if the ticket summary has exactly between {min} and {max} characters.\n",
    "If the number of characters is exactly in this range, return the original summary.\n",
    "Otherwise, recommend a new summary that number of characters is exactly in this range based on the information of the description field for the ticket.\n",
    "\n",
    "### Context ###\n",
    "Data: {ticket}\n",
    "\n",
    "Given the following answers, which one is the best, or can you synthesize a better one?\n",
    "Return the prompt type which provides the best answer with a short one sentence explanation.\n",
    "{answers}\"\"\"\n",
    "\n",
    "sumLen_consensus_prompt = PromptTemplate(template= sumLen_consensus_temp, input_variables= [\"ticket\", \"role\", \"min\", \"max\", \"answers\"])\n",
    "\n",
    "sumLen_consensus_prompt.save(\"prompts/summary/summaryLengthConsensusPrompt_V1.0.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen_consensus_form_temp = \"\"\"### Instruction ###\n",
    "You receive the prompt type, the answer, the new summary and an explanation, regarding the questions of which of a set of summarys is the best.\n",
    "Return the given Data in the following format in JSON:\n",
    "\"<prompt_type>\": \"<The prompt type, or NEW if not provided>\"\n",
    "\"<summary>\": \"<The new and recommended summary>\"\n",
    "\"<explanation>\": \"<The short explanation>\"\n",
    "\n",
    "### Context ###\n",
    "Data: {answer}\"\"\"\n",
    "\n",
    "sumLen_consensus_form_prompt = PromptTemplate(template= sumLen_consensus_form_temp, input_variables= [\"answer\"])\n",
    "\n",
    "sumLen_consensus_form_prompt.save(\"prompts/summary/summaryLengthConsensusFormatPrompt_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Structure Prompt (Best Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Best Practice: Description Completeness\n",
    "The received ticket is from type \"{issue_type}\".\n",
    "Your Task is to check if the \"Description\" field of the ticket is complete.\n",
    "We define a description as complete if it contains the following pieces:\n",
    "{content}\n",
    "Role: abstract behavior of actors in the system context; describes who uses the system.\n",
    "Task: specific things that must be done to achieve goals; solution or function of the problem.\n",
    "Capability: the ability of actors to achieve goals based on certain conditions and events.\n",
    "Goal: a condition or a circumstance desired by stakeholders or actors; describes the problem domain or the impact of solving the problem.\n",
    "\n",
    "The general format of a {issue_type} can be expressed as follows: \n",
    "{strucutre}\n",
    "As a <Role>, I want <Task>, So that <Goal>.\n",
    "\n",
    "If the description is complete, return the original description.\n",
    "Otherwise, recommend a new description for the ticket.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descComp_temp = \"\"\"Best Practice: Arbitrary Structure\n",
    "The received ticket is from the superior type \"{issue_type}\".\n",
    "Your Task is to check if the \"Description\" field of the ticket contains all components of a given structure.\n",
    "The components of the desired structure are described as follows:\n",
    "{structure_desc}\n",
    "\n",
    "The desired strucutre of a {issue_type} is as follows: \n",
    "{strucutre}\n",
    "\n",
    "If the description is conatins all components, return the original description.\n",
    "If the description contains nothing, return \"No description provided.\".\n",
    "Otherwise, recommend a new description, suitable to the desired structure for the ticket.\n",
    "\"\"\"\n",
    "\n",
    "descComp_prompt = PromptTemplate(template= descComp_temp, input_variables= [\"issue_type\", \"structure_desc\", \"structure\"])\n",
    "\n",
    "descComp_prompt.save(\"prompts/arbitraryStructure/arbitraryStructurePrompt_V1.0.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output format prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descComp_format_temp = \"\"\"### Instruction ###\n",
    "You receive a revised ticket from an issue tracker in JSON format.\n",
    "Return the given Data in the following format in JSON:\n",
    "\"<violation_predicted>\": \"<TRUE, if a violation was detectd, otherwise FALSE>\"\n",
    "\"<description_old>\": \"<Old version of the content of the description field>\"\n",
    "\"<description_new>\": \"<New version of the content of the description field>\"\n",
    "\n",
    "If the description was not changed, return the original description in the \"<description_new>\" field.\n",
    "Don't return any source code or code snippets.\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_ticket}\"\"\"\n",
    "\n",
    "descComp_format_prompt = PromptTemplate(template= descComp_format_temp, input_variables= [\"revised_ticket\"])\n",
    "\n",
    "descComp_format_prompt.save(\"prompts/arbitraryStructure/arbitraryStructureFormatPrompt_V1.0.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Speech Prompt (Best Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we find that including the term “ethnic slur” in our prompt greatly improves the performance because this term specifies the victims to be a group. Using\n",
    "“offensive or toxic content” as prompt doesn’t indi- cate the target of the victim, and thus leads to poor performance. \n",
    "\n",
    "LLMs cannot understand “hatespeech” is targeting a specific group, and thus also leads to poor performance.\n",
    "\n",
    "it seems that explic- itly saying “intend to offend\" is useful in surfacing out the LLMs’ underlying self-diagnosing abilities\n",
    "\n",
    "(Wang & Chang 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicSpeech_temp = \"\"\"Best Practice: Toxic Speech Detection\n",
    "Your task is to check the comments section for toxic speech.\n",
    "If you detect toxic speech, return the comment, the author, the datetime, and give a recommendation how to updated the comment.\"\"\"\n",
    "\n",
    "toxicSpeech_prompt = PromptTemplate(template= toxicSpeech_temp, input_variables= [])\n",
    "\n",
    "toxicSpeech_prompt.save(\"prompts/toxicSpeech/toxicSpeechPrompt_V1.5.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output format prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicSpeech_format_temp = \"\"\"### Instruction ###\n",
    "You receive a list of comments from a revised ticket from an issue tracker.\n",
    "Return the given Data for each entry in the following format in JSON:\n",
    "\"comment\": \"<content of the original comment>\",\n",
    "\"author\": \"<name of the original comment>\",\n",
    "\"datetime\": \"<datetime of the original comment>\",\n",
    "\"recommendation\": \"<only the revised string of the new comment>\"\n",
    "\"reason\": \"<reason for the recommendation>\"\n",
    "If the list is empty return \"no toxic speech detected\".\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_ticket} \"\"\"\n",
    "\n",
    "toxicSpeech_format_prompt = PromptTemplate(template= toxicSpeech_format_temp, input_variables= [\"revised_ticket\"])\n",
    "\n",
    "toxicSpeech_format_prompt.save(\"prompts/toxicSpeech/toxicSpeechFormatPrompt_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internationalization Prompt (Best Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprache variable machen\n",
    "internationalization_temp = \"\"\"Best Practice: Internationalization\n",
    "Bug reports not written in English are often closed immediately or ignored by developers.\n",
    "Your task is to check if the \"Summary\", \"Description\", or \"Comments\" fields of the ticket are written in English.\n",
    "If some fields of the ticket are not written in English, provide a translation.\n",
    "Return only the revised ticket and nothing else.\"\"\"\n",
    "\n",
    "internationalization_prompt = PromptTemplate(template= internationalization_temp, input_variables= [])\n",
    "\n",
    "internationalization_prompt.save(\"prompts/internationalization/internationalizationPrompt_V1.5.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output format prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internationalization_format_temp = \"\"\"### Instruction ###\n",
    "You receive a revised ticket from an issue tracker in JSON format.\n",
    "Return the given Data in the following format in JSON:\n",
    "\"<field>\": \"<content of the field>\"\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_ticket}\"\"\"\n",
    "\n",
    "internationalization_format_prompt = PromptTemplate(template= internationalization_format_temp, input_variables= [\"revised_ticket\"])\n",
    "\n",
    "internationalization_format_prompt.save(\"prompts/internationalization/internationalizationFormatPrompt_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug Report Structure Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe more of this structure:\n",
    "- https://bugzilla.mozilla.org/page.cgi?id=bug-writing.html\n",
    "- https://bugs.eclipse.org/bugs/page.cgi?id=bug-writing.html\n",
    "- https://help.minecraft.net/hc/en-us/articles/4408887473421-Mojang-Bug-Tracker-Guidelines-and-FAQ#h_01FF6B0GPYQJR44QAMJN7YEXXR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_temp = \"\"\"'Bug Report Structure':\n",
    "The description field is important for the bug report, as it describes the issue in detail.\n",
    "\n",
    "### Description Components ###\n",
    "The description of a bug report should contain the following important components:\n",
    "Overview: A more detailed description of the context when the error occurred.\n",
    "Steps to reproduce: A list of steps to reproduce the error.\n",
    "Actual result: The result that is currently produced.\n",
    "Expected result: The result that is expected.\n",
    "\n",
    "Additionally, the description could contain the following components:\n",
    "Stack Traces: A list of relevant stack traces for the bug report.\n",
    "Build date and hardware: The build date and hardware, as well as when and where the bug occurred.\n",
    "Additional information: Additional information relevant to the bug report (screenshots, videos, or other files).\n",
    "\n",
    "### Task ###\n",
    "We want to determine the degree of completeness of the original/old description.\n",
    "The important components of a description are an overview/summary, steps to reproduce, the actual result and the expected result.\n",
    "The degree is between 1 and 3:\n",
    "1: The original/old description contains all important components or has only been slightly modified.\n",
    "2: The original/old description contains 2 or 3 components but is not complete.\n",
    "3: The original/old description is only a text without a clear structure and contains at most  1 important component.\n",
    "\n",
    "If the description field is missing some important parts, we want to rearrange the description field accordingly.\n",
    "Return the degree of completeness, the revised description, and a short two-sentence explanation for the changes made/not made.\n",
    "\n",
    "The bug report is in JSON format.\n",
    "Bug report: {bug_report}\"\"\"\n",
    "\n",
    "bugReportStructure_prompt = PromptTemplate(template=bugReportStructure_temp, input_variables=[\"bug_report\"])\n",
    "\n",
    "bugReportStructure_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_V2.1.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-Shot CoT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_0ShotCoT_temp = \"\"\"'Bug Report Structure':\n",
    "The description field is important for the bug report, as it describes the issue in detail.\n",
    "\n",
    "### Description Components ###\n",
    "The description of a bug report should contain the following important components:\n",
    "Overview: A more detailed description of the context when the error occurred.\n",
    "Steps to reproduce: A list of steps to reproduce the error.\n",
    "Actual result: The result that is currently produced.\n",
    "Expected result: The result that is expected.\n",
    "\n",
    "Additionally, the description could contain the following components:\n",
    "Stack Traces: A list of relevant stack traces for the bug report.\n",
    "Build date and hardware: The build date and hardware, as well as when and where the bug occurred.\n",
    "Additional information: Additional information relevant to the bug report (screenshots, videos, or other files).\n",
    "\n",
    "### Task ###\n",
    "We want to determine the degree of completeness of the original/old description.\n",
    "The important components of a description are an overview/summary, steps to reproduce, the actual result and the expected result.\n",
    "The degree is between 1 and 3:\n",
    "1: The original/old description contains all important components or has only been slightly modified.\n",
    "2: The original/old description contains 2 or 3 components but is not complete.\n",
    "3: The original/old description is only a text without a clear structure and contains at most 1 important component.\n",
    "\n",
    "If the description field is missing some important parts, we want to rearrange the description field accordingly.\n",
    "Return the degree of completeness, the revised description, and a short two-sentence explanation for the changes made/not made.\n",
    "\n",
    "The bug report is in JSON format.\n",
    "Bug report: {bug_report}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "bugReportStructure_0ShotCoT_prompt = PromptTemplate(template= bugReportStructure_0ShotCoT_temp, input_variables= [\"bug_report\"])\n",
    "\n",
    "bugReportStructure_0ShotCoT_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_0ShotCoT_V1.0.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Approvals history items with deleted status' are breaking the Bento view',\n",
    "'Description':'Approvals history items with deleted status' are breaking the Bento view\\r\\nh3. Steps to Reproduce\\r\\n# Create an approval status on your workflow.\\r\\n# Transition the ticket and approve it.\\r\\n# Now, delete the status created on the first step.\\r\\n\\r\\nh3. Expected Results\\r\\nYou should be able to access the ticket in the new issue view.\\r\\n\\r\\nh3. Actual Results\\r\\n\\r\\nThe ticket does not load as the ticket history points to the deleted status and an exception is thrown in the logs.\\r\\n\\r\\nh3. Workaround\\r\\nAccess the ticket via the new issue view or clone the ticket.'}}\"\"\",\n",
    "        \"output\": \"\"\"Degree of Completeness: 1\n",
    "\n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "Approvals history items with deleted status' are breaking the Bento view.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Create an approval status on your workflow.\n",
    "2. Transition the ticket and approve it.\n",
    "3. Now, delete the status created on the first step.\n",
    "\n",
    "Actual result\n",
    "The ticket does not load as the ticket history points to the deleted status and an exception is thrown in the logs.\n",
    "\n",
    "Expected result\n",
    "You should be able to access the ticket in the new issue view.\n",
    "\n",
    "Additional information\n",
    "a) Workaround: Access the ticket via the new issue view or clone the ticket.\n",
    "```\n",
    "\n",
    "Explanation: The description starts with an overview, even if it does not have a corresponding heading. The degree of completeness is therefore 1 as the original description contains all the important parts and has only been slightly restructured.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"{{'Summary':'Text formatting does not work in the Visual mode of the Comment field in a Transition screen',\n",
    "'Description':'h3. Problem definition\\r\\nText formatting like bold, italic, underline and color do not work for the Comment added in the Visual mode of a transition screen.\\r\\n\\r\\nh3. Steps to reproduce the issue\\r\\n# Configure a screen for any transition for a workflow associated to a Service Desk project.\\r\\n# Ensure that the *Rich text editor* is enabled.\\r\\n# Transition a Service Desk issue and on the pop-up screen, use the *Visual mode*.\\r\\n# Type something and make it bold\\/italic\\/underlined\\/colored.\\r\\n\\r\\nh3. Expected result\\r\\nThe comment will be shown with the text formatting applied.\\r\\n\\r\\nh3. Actual result\\r\\nThe comment is highlighted without any text formatting applied.\\r\\n\\r\\nh3. Notes\\r\\n# This does not happen on issue view, only transition screen.\\r\\n# This does not happen to non-Service Desk projects.'}}\"\"\",\n",
    "        \"output\": \"\"\"Degree of Completeness: 1\n",
    "\n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "Text formatting like bold, italic, underline and color do not work for the Comment added in the Visual mode of a transition screen.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Configure a screen for any transition for a workflow associated to a Service Desk project.\n",
    "2. Ensure that the *Rich text editor* is enabled.\n",
    "3. Transition a Service Desk issue and on the pop-up screen, use the *Visual mode*.\n",
    "4. Type something and make it bold\\/italic\\/underlined\\/colored.\n",
    "\n",
    "Actual result\n",
    "The comment is highlighted without any text formatting applied.\n",
    "\n",
    "Expected result\n",
    "The comment will be shown with the text formatting applied.\n",
    "\n",
    "Additional information\n",
    "a) This does not happen on issue view, only transition screen.\n",
    "b) This does not happen to non-Service Desk projects.\n",
    "```\n",
    "\n",
    "Explanation: The original description contains all the important components, including the overview, steps to reproduce, the actual result, and the expected result. The degree of completeness is therefore 1 as the original description contains all important parts including the topics for separation and has only been slightly restructured.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Cmake warning spam in build of maint-4952-v-t-u (coroutine)',\n",
    "'Description':'h1. Steps to Reproduce\\nBuilding opensource maint-4952-v-t-u \\n\\nh1. Actual Behavior\\nBuilt HTTPcoroutine on windows and it generated a lot of cmake warnings mostly boost in cef.\\n\\nh1. Expected Behavior\\nNot have the warnings \\n\\nh1. Other information\\nThis warning is for project developers.  Use -Wno-dev to suppress it.\\r\\nI could add -Wno-dev. But I think these warnings should be corrected.'}}\"\"\",\n",
    "        \"output\": \"\"\"Degree of Completeness: 2\n",
    "\n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "During the build of opensource maint-4952-v-t-u, a lot of CMake warnings are generated. \n",
    "\n",
    "Steps to reproduce\n",
    "1. Building opensource maint-4952-v-t-u\n",
    "\n",
    "Actual result\n",
    "Built HTTPcoroutine on windows and it generated a lot of cmake warnings mostly boost in cef.\n",
    "\n",
    "Expected result\n",
    "Not have the warnings\n",
    "\n",
    "Additional information\n",
    "a) This warning is for project developers. \n",
    "b) Use -Wno-dev to suppress it. But I think these warnings should be corrected.\n",
    "```\n",
    "\n",
    "Explanation: The original description starts directly with the steps to reproduce, followed by the actual result and the expected result, but there are no indications for the overview. The degree of completeness is therefore 2 as the missing overview component has to be generated.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'its still demo',\n",
    "'Description':'i just buy paceofcarde for 100 zl becose i want to buy minecraft but when i go to the webside and i dowlond the game it show that is demo i tried to dowlond one more time i chec i this the same username and password but its stil nothing pleas help'}}\"\"\",\n",
    "        \"output\": \"\"\"Degree of Completeness: 3\n",
    "        \n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "I want to buy minecraft, but after the download it shows that it is a demo version. I already checked the username and password and repeated the download but it still is the demo version.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Go to the minecraft website\n",
    "2. Download the game\n",
    "\n",
    "Actual result\n",
    "The demo version of the game is downloaded\n",
    "\n",
    "Expected result\n",
    "The full version of the game should be downloaded\n",
    "```\n",
    "\n",
    "Explanation: The original description is plain text with an explanation of what the person did, but no structure was given. The degree of completeness is therefore 3 because the original description was a simple text without any headings or structure.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Exceptions raised during AM launch don't trigger exit code AM failures',\n",
    "'Description':'It looks like exceptions raised in AM launch failure, while being caught, don't have their exit code picked up. Any launch-time failure should result in an exit code being returned, which then tells the YARN to restart it.'}}\"\"\",\n",
    "        \"output\": \"\"\"Degree of Completeness: 3\n",
    "        \n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "The exit code of exceptions raised in AM launch failure is not picked up. Even if the exception is caught correctly.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Launch AM\n",
    "2. Observe the exceptions raised during the launch\n",
    "\n",
    "Actual result\n",
    "Exceptions raised in AM launch failure, don't have their exit code picked up, while being caught.\n",
    "\n",
    "Expected result\n",
    "Any launch-time failure should result in an exit code being returned, which then tells the YARN to restart it.\n",
    "```\n",
    "\n",
    "Explanation: Even if the original description assumes that it contains the expected behavior and an overview, there is no clear structure through topics. The degree of completeness is therefore 3 as the original description was a plain text without any headings or structure.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"], \n",
    "    template=\"\"\"Bug Report: {input}\n",
    "\n",
    "Answer: {output}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_examples_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix= \"\"\"Bug report: {bug_report}\n",
    "\n",
    "Answer: \"\"\",\n",
    "    input_variables=[\"bug_report\"]\n",
    ")\n",
    "\n",
    "bugReportStructure_examples_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_FewShot_Examples_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_task_temp = \"\"\"'Bug Report Structure':\n",
    "The description field is important for the bug report, as it describes the issue in detail.\n",
    "\n",
    "### Description Components ###\n",
    "The description of a bug report should contain the following important components:\n",
    "Overview: A description of the context/problem, a summary, te requirement, or a problem statement. This is usual the beginning of the description. Keywords for overview heading are: overview, summary, description, problem;\n",
    "Steps to reproduce: A list of steps to reproduce, or the scenario to produce the error. Keywords for steps to reproduce heading are: steps, reproduce, scenario, how to;\n",
    "Actual result: The result that is currently produced, the actual behaviour, or what actual happens. Keywords for actual result heading are: actual, result, behaviour, happens;\n",
    "Expected result: The result that was expected, the expected behaviour, or what should happen. Keywords for expected result heading are: expected, result, behaviour, should.\n",
    "\n",
    "Additionally, the description could contain the following components:\n",
    "Stack Traces: A list of relevant stack traces for the bug report.\n",
    "Build date and hardware: The build date and hardware, as well as when and where the bug occurred.\n",
    "Additional information: Additional information relevant to the bug report, including screenshots, videos, or other files.\n",
    "\n",
    "### Task ###\n",
    "We want to determine the degree of completeness of the original/old description based on the provided context of the description field.\n",
    "The important components of a description are an overview, steps to reproduce, the actual result and the expected result.\n",
    "We define a clear structure when we have appropriate headings for each important component in the description, except for the overview component.\n",
    "Even if the overview component does not have a heading, but can be assumed as overview it is considered to be present and complete.\n",
    "The degree is between 1 and 3:\n",
    "1: The original/old description contains all important components or has only been rearranged.\n",
    "2: The original/old description contains a minimum of 2 components but is not complete.\n",
    "3: The original/old description is a plain text without a clear structure.\n",
    "\n",
    "If the description field is missing some important parts, we want to rearrange the description field accordingly.\n",
    "Return the degree of completeness, the revised description, and a short two-sentence explanation for the changes made.\n",
    "\n",
    "The bug report is in JSON format.\n",
    "{examples}\"\"\"\n",
    "\n",
    "bugReportStructure_task_prompt = PromptTemplate(template=bugReportStructure_task_temp, input_variables=[\"examples\"])\n",
    "\n",
    "bugReportStructure_task_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_FewShot_V2.11.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Cmake warning spam in build of maint-4952-v-t-u (coroutine)',\n",
    "'Description':'Steps to Reproduce\\nBuilding opensource maint-4952-v-t-u \\n\\nActual Behavior\\nBuilt HTTPcoroutine on windows and it generated a lot of cmake warnings mostly boost in cef.\\n\\nExpected Behavior\\nNot have the warnings \\n\\nOther information\\nThis warning is for project developers.  Use -Wno-dev to suppress it.\\r\\nI could add -Wno-dev. But I think these warnings should be corrected.'}}\"\"\",\n",
    "        \"output\": \"\"\"We want to check if all important components are present in the description field of the ticket.\n",
    "First we want to check if an overview is present. There is no keyword or heading that indicates an overview. The description starts directly with the steps to reproduce. The overview component is missing.\n",
    "Secondly, we want to check if the steps to reproduce are present. The steps to reproduce are clearly defined under the heading \"Steps to Reproduce\". Corresponding steps are listed. The steps to reproduce component is present.\n",
    "Thirdly, we want to check if the actual result is present. The actual result is clearly defined under the heading \"Actual Behavior\". The actual result component is present.\n",
    "Lastly, we want to check if the expected result is present. The expected result is clearly defined under the heading \"Expected Behavior\". The expected result component is present.\n",
    "Additionally, we have the component \"Additional information\" highlighted by the header \"Other information\". \n",
    "\n",
    "Three important components are available, but there is no overview, so the Degree of Completeness is: 2 \n",
    "\n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "During the build of opensource maint-4952-v-t-u, a lot of CMake warnings are generated. \n",
    "\n",
    "Steps to reproduce\n",
    "1. Building opensource maint-4952-v-t-u\n",
    "\n",
    "Actual result\n",
    "Built HTTPcoroutine on windows and it generated a lot of cmake warnings mostly boost in cef.\n",
    "\n",
    "Expected result\n",
    "Not have the warnings\n",
    "\n",
    "Additional information\n",
    "a) This warning is for project developers. \n",
    "b) Use -Wno-dev to suppress it. But I think these warnings should be corrected.\n",
    "```\n",
    "\n",
    "Explanation: The original description starts directly with the steps to reproduce, followed by the actual result and the expected result, but there are no indications for the overview. The degree of completeness is therefore 2 as the missing overview component has to be generated.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Approvals history items with deleted status' are breaking the Bento view',\n",
    "'Description':'Approvals history items with deleted status' are breaking the Bento view\\r\\nh3. Steps to Reproduce\\r\\n# Create an approval status on your workflow.\\r\\n# Transition the ticket and approve it.\\r\\n# Now, delete the status created on the first step.\\r\\n\\r\\nh3. Expected Results\\r\\nYou should be able to access the ticket in the new issue view.\\r\\n\\r\\nh3. Actual Results\\r\\n\\r\\nThe ticket does not load as the ticket history points to the deleted status and an exception is thrown in the logs.\\r\\n\\r\\nh3. Workaround\\r\\nAccess the ticket via the new issue view or clone the ticket.'}}\"\"\",\n",
    "        \"output\": \"\"\"We want to check if all important components are present in the description field of the ticket.\n",
    "First, we want to check if an overview is present. No keyword or heading indicates an overview, but the first sentence of the description can be interpreted as an overview by the ticket context. The overview component is present.\n",
    "Secondly, we want to check if the steps to reproduce are present. The steps to reproduce are clearly defined under the heading \"Steps to Reproduce,\" which is also recognizable by its h1.-heading. Corresponding steps are listed. The steps to reproduce component is present.\n",
    "Thirdly, we want to check if the actual result is present. The actual result is clearly defined under the heading \"Actual Results\" which is also recognizable by its h1.-heading. The actual result component is present.\n",
    "Lastly, we want to check if the expected result is present. The expected result is clearly defined under the heading \"Expected Results\", which is also recognizable by its h1.-heading. The expected result component is present.\n",
    "        \n",
    "All important components are available so the Degree of Completeness is: 1\n",
    "\n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "Approvals history items with deleted status' are breaking the Bento view.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Create an approval status on your workflow.\n",
    "2. Transition the ticket and approve it.\n",
    "3. Now, delete the status created on the first step.\n",
    "\n",
    "Actual result\n",
    "The ticket does not load as the ticket history points to the deleted status and an exception is thrown in the logs.\n",
    "\n",
    "Expected result\n",
    "You should be able to access the ticket in the new issue view.\n",
    "\n",
    "Additional information\n",
    "a) Workaround: Access the ticket via the new issue view or clone the ticket.\n",
    "```\n",
    "\n",
    "Explanation: The descriptions contains an overview, steps to reproduce, an actual result and an expected result so all important components are included. The degree of completeness is therefore 1 as the original description contains all the important parts and has only been slightly restructured.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\":\"\"\"{{'Summary':'Exceptions raised during AM launch don't trigger exit code AM failures',\n",
    "'Description':'It looks like exceptions raised in AM launch failure, while being caught, don't have their exit code picked up. Any launch-time failure should result in an exit code being returned, which then tells the YARN to restart it.'}}\"\"\",\n",
    "        \"output\": \"\"\"We want to check if all important components are present in the description field of the ticket.\n",
    "First, we want to check if an overview is available. There is no keyword or heading that indicates an overview. The whole description is plain text describing a problem, which could be interpreted as the overview component. The overview component is present.\n",
    "Secondly, we want to check if the steps to reproduce are present. There is no keyword or heading that indicates the steps to reproduce. The steps to reproduce component is missing.\n",
    "Thirdly, we want to check if the actual result is present. No keyword or heading indicates the actual result, even if a potential result can be inferred from the overview text. The actual result component is missing.\n",
    "Lastly, we want to check if the expected result is present. No keyword or heading indicates the expected result, even if a potential expected result can be inferred from the overview text. The expected result component is missing.\n",
    "\n",
    "Besides a plain text as the overview no structure was provided and important components are missing so the Degree of Completeness is: 3\n",
    "        \n",
    "Revised Description:\n",
    "```\n",
    "Overview\n",
    "The exit code of exceptions raised in AM launch failure is not picked up. Even if the exception is caught correctly.\n",
    "\n",
    "Steps to reproduce\n",
    "1. Launch AM\n",
    "2. Observe the exceptions raised during the launch\n",
    "\n",
    "Actual result\n",
    "Exceptions raised in AM launch failure, don't have their exit code picked up, while being caught.\n",
    "\n",
    "Expected result\n",
    "Any launch-time failure should result in an exit code being returned, which then tells the YARN to restart it.\n",
    "```\n",
    "\n",
    "Explanation: Even if the original description assumes that it contains the expected behavior and an overview, there is no clear structure through topics. The degree of completeness is therefore 3 as the original description was a plain text without any headings or structure.\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"], \n",
    "    template=\"\"\"Bug Report: {input}\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "{output}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_examplesCoT_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix= \"\"\"Bug report: {bug_report}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\",\n",
    "    input_variables=[\"bug_report\"]\n",
    ")\n",
    "\n",
    "bugReportStructure_examplesCoT_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_FewShotCoT_Examples_V1.2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_task_temp = \"\"\"'Bug Report Structure':\n",
    "The description field is important for the bug report, as it describes the issue in detail.\n",
    "\n",
    "### Description Components ###\n",
    "The description of a bug report should contain the following important components:\n",
    "Overview: A description of the context/problem, a summary, the requirement, or a problem statement. This is usually the beginning of the description. Keywords for overview heading are: overview, summary, description, problem;\n",
    "Steps to reproduce: A list of steps to reproduce, or the scenario to produce the error. Keywords for steps to reproduce heading are: steps, reproduce, scenario, how to;\n",
    "Actual result: The result that is currently produced, the actual behaviour, or what actual happens. Keywords for actual result heading are: actual, result, behaviour, happens;\n",
    "Expected result: The result that was expected, the expected behaviour, or what should happen. Keywords for expected resul heading are: expected, result, behaviour, should.\n",
    "\n",
    "Additionally, the description could contain the following components:\n",
    "Stack Traces: A list of relevant stack traces for the bug report.\n",
    "Build date and hardware: The build date and hardware, as well as when and where the bug occurred.\n",
    "Additional information: Additional information relevant to the bug report, including screenshots, videos, or other files.\n",
    "\n",
    "### Task ###\n",
    "We want to determine the degree of completeness of the original/old description based on the provided context of the description field.\n",
    "The important components of a description are an overview, steps to reproduce, the actual result and the expected result.\n",
    "We define a clear structure when we have appropriate headings for each important component in the description, except fot the overview component.\n",
    "Even if the overview component does not have a heading, it is considered to be present and complete.\n",
    "The degree is between 1 and 3:\n",
    "1: The original/old description contains all important components or has only been slightly changed.\n",
    "2: The original/old description contains 2 or 3 components but is not complete.\n",
    "3: The original/old description is a plain text without a clear structure.\n",
    "\n",
    "If the description field is missing some important parts, we want to rearrange the description field accordingly.\n",
    "Return the degree of completeness, the revised description, and a short two-sentence explanation for the changes made.\n",
    "\n",
    "The bug report is in JSON format.\n",
    "{examples}\",\n",
    "\"\"\"\n",
    "\n",
    "bugReportStructure_task_prompt = PromptTemplate(template=bugReportStructure_task_temp, input_variables=[\"examples\"])\n",
    "\n",
    "bugReportStructure_task_prompt.save(\"prompts/bugReportStructure/bugReportStructurePrompt_FewShotCoT_V1.3.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Format Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStructure_format_temp = \"\"\"### Instruction ###\n",
    "You receive a revised bug report from an issue tracker.\n",
    "Return the given data in the following format in JSON:\n",
    "\"<smell_degree>\": \"<This is the degree of change, between 1 and 3>\"\n",
    "\"<description_new>\": \"<The received recommended and revised description>\"\n",
    "\"<reason>\": \"<The received two sentence explanation for the change and the degree of change>\"\n",
    "\n",
    "If the description was not changed, return the original and unchanged description in the \"<description_new>\" field.\n",
    "Don't return any source code or code snippets.\n",
    "\n",
    "### Context ###\n",
    "Data: {revised_bug_report}\"\"\"\n",
    "\n",
    "bugReportStructure_format_prompt = PromptTemplate(template= bugReportStructure_format_temp, input_variables= [\"revised_bug_report\"])\n",
    "\n",
    "bugReportStructure_format_prompt.save(\"prompts/bugReportStructure/bugReportStructureFormatPrompt_V2.2.1.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
