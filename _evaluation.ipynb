{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from sklearn import metrics\n",
    "from rouge import Rouge\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions ####\n",
    "##################\n",
    "\n",
    "def getOriginalSummary(uri):\n",
    "    with open(uri) as f:\n",
    "        ticket = json.load(f)\n",
    "\n",
    "    sum = \"\"    \n",
    "    if ticket['Summary'] is not None:\n",
    "        sum = ticket['Summary']\n",
    "    \n",
    "    return sum    \n",
    "\n",
    "def computeROUGE(reference, predicted):\n",
    "    rouge = Rouge()\n",
    "    scores = [rouge.get_scores(new, old) for old, new in zip(reference, predicted)]\n",
    "\n",
    "    total_scores = {\"rouge-1\": {\"f\": 0, \"p\": 0, \"r\": 0}, \"rouge-2\": {\"f\": 0, \"p\": 0, \"r\": 0}, \"rouge-l\": {\"f\": 0, \"p\": 0, \"r\": 0}}\n",
    "\n",
    "    for score in scores:\n",
    "        for key in total_scores.keys():\n",
    "            total_scores[key]['f'] += score[0][key]['f']\n",
    "            total_scores[key]['p'] += score[0][key]['p']\n",
    "            total_scores[key]['r'] += score[0][key]['r']\n",
    "\n",
    "    for key in total_scores.keys():\n",
    "        total_scores[key]['f'] /= len(scores)\n",
    "        total_scores[key]['p'] /= len(scores)\n",
    "        total_scores[key]['r'] /= len(scores)\n",
    "    \n",
    "    return total_scores  \n",
    "\n",
    "def createEvaluationCSV(directory):\n",
    "    evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"reruns\", \"ticket_uri\", \"output_uri\", \"violation_actual\", \"violation_predicted\", \"summary_original\", \"length_original\", \"summary_old\", \"length_old\", \"summary_new\", \"length_new\", \"correction_in_range\"])\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        outputUri = directory + filename\n",
    "\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        with open(outputUri) as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        if (len(result[\"output\"][\"summary_new\"]) <= 70) & (len(result[\"output\"][\"summary_new\"]) >= 39):\n",
    "            in_range = \"TRUE\"\n",
    "        else:\n",
    "            in_range = \"FALSE\"\n",
    "\n",
    "        summary_org = getOriginalSummary(result[\"input_data\"][\"ticket_uri\"])\n",
    "\n",
    "        new_row = {\n",
    "            'jira': result[\"input_data\"][\"jira\"], \n",
    "            'ticketId': result[\"input_data\"][\"id\"], \n",
    "            'evolution': result[\"input_data\"][\"evolution\"], \n",
    "            'reruns': result[\"reruns\"],\n",
    "            'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "            'output_uri': outputUri, \n",
    "            'violation_actual': result[\"violation_actual\"], \n",
    "            'violation_predicted': result[\"output\"][\"violation_predicted\"],\n",
    "            'summary_original': summary_org,\n",
    "            'length_original': len(summary_org),\n",
    "            'summary_old': result[\"output\"][\"summary_old\"],\n",
    "            'length_old': len(result[\"output\"][\"summary_old\"]), \n",
    "            'summary_new': result[\"output\"][\"summary_new\"],\n",
    "            'length_new': len(result[\"output\"][\"summary_new\"]), \n",
    "            'correction_in_range': in_range\n",
    "            }\n",
    "\n",
    "        evalDF.loc[len(evalDF)]=new_row\n",
    "\n",
    "    # Save data to csv\n",
    "    evalDF.to_csv(directory + \"evaluatedSummarys.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/0Shot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")\n",
    "evalDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] == 0]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f05 = metrics.fbeta_score(actual, predicted, beta=0.5, pos_label=True)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"Smell\", \"No Smell\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "## Oben Links = TP (Verstoß wird erkannt und liegt vor)\n",
    "## Unten Rechts = TN (Verstoß wird nicht erkannt und liegt nicht vor)\n",
    "## Oben Rechts = FN (Verstoß wird nicht erkannt obwohl einer vorliegt)\n",
    "## Unten Links = FP (Verstoß wird erkannt obwohl keiner vorliegt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F.5: \" + str(f05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(successRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_original\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores:\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/FewShot/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 0]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f05 = metrics.fbeta_score(actual, predicted, beta=0.5, pos_label=True)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"Smell\", \"No Smell\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F.5: \" + str(f05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(successRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_original\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores (Sum):\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/0ShotCoT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 0]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f05 = metrics.fbeta_score(actual, predicted, beta=0.5, pos_label=True)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"Smell\", \"No Smell\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F.5: \" + str(f05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(successRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_old\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores (Sum):\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/update/gpt-4-0125-preview/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"reruns\", \"ticket_uri\", \"output_uri\", \"violation_actual\", \"violation_predicted\", \"change_actual\", \"change_predicted\", \"success\"])\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    outputUri = directory + filename\n",
    "\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(outputUri) as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    new_row = {\n",
    "        'jira': result[\"input_data\"][\"jira\"], \n",
    "        'ticketId': result[\"input_data\"][\"id\"], \n",
    "        'evolution': result[\"input_data\"][\"evolution\"],\n",
    "        'reruns': result[\"reruns\"],\n",
    "        'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "        'output_uri': outputUri, \n",
    "        'violation_actual': result[\"violation_actual\"], \n",
    "        'violation_predicted': result[\"output\"][\"violation_predicted\"],\n",
    "        'change_actual': result[\"reason\"],\n",
    "        'change_predicted': result[\"output\"][\"fields\"],\n",
    "        'success': None\n",
    "        }\n",
    "\n",
    "    evalDF.loc[len(evalDF)]=new_row    \n",
    "\n",
    "# Save data to csv\n",
    "evalDF.to_csv(directory + \"updateSummarys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=\"TRUE\")\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=\"TRUE\")\n",
    "f05 = metrics.fbeta_score(actual, predicted, beta=0.5, pos_label=\"TRUE\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[\"TRUE\", \"FALSE\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"TRUE\", \"FALSE\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F05: \" + str(f05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"updateSummarys_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successRate = evalDF[\"success\"].value_counts(normalize=True)[True]\n",
    "print(\"Success rate: \" + str(successRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
