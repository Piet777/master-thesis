{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(\"./evaluation/evaluatedSummarys.csv\"):\n",
    "#     evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"outputUri\", \"violation_actual\", \"violation_predicted\", \"summary_old\", \"length_old\", \"summary_new\", \"length_new\", \"correction_in_range\"])\n",
    "# else:\n",
    "#     evalDF = pd.read_csv(\"./evaluation/evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/\"\n",
    "\n",
    "evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"output_uri\", \"violation_actual\", \"violation_predicted\", \"summary_old\", \"length_old\", \"summary_new\", \"length_new\", \"correction_in_range\"])\n",
    "\n",
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    outputUri = directory + filename\n",
    "\n",
    "    with open(outputUri) as f:\n",
    "        result = json.load(f)\n",
    "    \n",
    "    actual = np.append(actual, result[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, result[\"output\"][\"violation_predicted\"])\n",
    "\n",
    "    if (len(result[\"output\"][\"summary_new\"]) <= 70) & (len(result[\"output\"][\"summary_new\"]) >= 39):\n",
    "        in_range = \"TRUE\"\n",
    "    else:\n",
    "        in_range = \"FALSE\"\n",
    "\n",
    "    new_row = {\n",
    "        'jira': result[\"input_data\"][\"jira\"], \n",
    "        'ticketId': result[\"input_data\"][\"id\"], \n",
    "        'evolution': result[\"input_data\"][\"evolution\"], \n",
    "        'output_uri': outputUri, \n",
    "        'violation_actual': result[\"violation_actual\"], \n",
    "        'violation_predicted': result[\"output\"][\"violation_predicted\"],\n",
    "        'summary_old': result[\"output\"][\"summary_old\"],\n",
    "        'length_old': len(result[\"output\"][\"summary_old\"]), \n",
    "        'summary_new': result[\"output\"][\"summary_new\"],\n",
    "        'length_new': len(result[\"output\"][\"summary_new\"]), \n",
    "        'correction_in_range': in_range\n",
    "        }\n",
    "\n",
    "    evalDF.loc[len(evalDF)]=new_row\n",
    "\n",
    "# Save data to csv\n",
    "evalDF.to_csv(\"./evaluation/evaluatedSummarys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[\"TRUE\", \"FALSE\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"TRUE\", \"FALSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
