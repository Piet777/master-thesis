{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from rouge import Rouge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions ###\n",
    "def computeROUGE(reference, predicted):\n",
    "    rouge = Rouge()\n",
    "    scores = [rouge.get_scores(new, old) for old, new in zip(reference, predicted)]\n",
    "\n",
    "    total_scores = {\"rouge-1\": {\"f\": 0, \"p\": 0, \"r\": 0}, \"rouge-2\": {\"f\": 0, \"p\": 0, \"r\": 0}, \"rouge-l\": {\"f\": 0, \"p\": 0, \"r\": 0}}\n",
    "\n",
    "    for score in scores:\n",
    "        for key in total_scores.keys():\n",
    "            total_scores[key]['f'] += score[0][key]['f']\n",
    "            total_scores[key]['p'] += score[0][key]['p']\n",
    "            total_scores[key]['r'] += score[0][key]['r']\n",
    "\n",
    "    for key in total_scores.keys():\n",
    "        total_scores[key]['f'] /= len(scores)\n",
    "        total_scores[key]['p'] /= len(scores)\n",
    "        total_scores[key]['r'] /= len(scores)\n",
    "    \n",
    "    return total_scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions ####\n",
    "##################\n",
    "\n",
    "def getOriginalSummary(uri):\n",
    "    with open(uri) as f:\n",
    "        ticket = json.load(f)\n",
    "\n",
    "    sum = \"\"    \n",
    "    if ticket['Summary'] is not None:\n",
    "        sum = ticket['Summary']\n",
    "    \n",
    "    return sum    \n",
    "\n",
    "def createEvaluationCSV(directory):\n",
    "    evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"reruns\", \"ticket_uri\", \"output_uri\", \"violation_actual\", \"violation_predicted\", \"summary_original\", \"length_original\", \"summary_old\", \"length_old\", \"summary_new\", \"length_new\", \"correction_in_range\"])\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        outputUri = directory + filename\n",
    "\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        with open(outputUri) as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        if (len(result[\"output\"][\"summary_new\"]) <= 70) & (len(result[\"output\"][\"summary_new\"]) >= 39):\n",
    "            in_range = \"TRUE\"\n",
    "        else:\n",
    "            in_range = \"FALSE\"\n",
    "\n",
    "        summary_org = getOriginalSummary(result[\"input_data\"][\"ticket_uri\"])\n",
    "\n",
    "        new_row = {\n",
    "            'jira': result[\"input_data\"][\"jira\"], \n",
    "            'ticketId': result[\"input_data\"][\"id\"], \n",
    "            'evolution': result[\"input_data\"][\"evolution\"], \n",
    "            'reruns': result[\"reruns\"],\n",
    "            'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "            'output_uri': outputUri, \n",
    "            'violation_actual': result[\"violation_actual\"], \n",
    "            'violation_predicted': result[\"output\"][\"violation_predicted\"],\n",
    "            'summary_original': summary_org,\n",
    "            'length_original': len(summary_org),\n",
    "            'summary_old': result[\"output\"][\"summary_old\"],\n",
    "            'length_old': len(result[\"output\"][\"summary_old\"]), \n",
    "            'summary_new': result[\"output\"][\"summary_new\"],\n",
    "            'length_new': len(result[\"output\"][\"summary_new\"]), \n",
    "            'correction_in_range': in_range\n",
    "            }\n",
    "\n",
    "        evalDF.loc[len(evalDF)]=new_row\n",
    "\n",
    "    # Save data to csv\n",
    "    evalDF.to_csv(directory + \"evaluatedSummarys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/0Shot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] == 0]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d', cbar=False, xticklabels=[\"Smell\", \"No Smell\"], yticklabels=[\"Smell\", \"No Smell\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "## Oben Links = TP (Verstoß wird erkannt und liegt vor)\n",
    "## Unten Rechts = TN (Verstoß wird nicht erkannt und liegt nicht vor)\n",
    "## Oben Rechts = FN (Verstoß wird nicht erkannt obwohl einer vorliegt)\n",
    "## Unten Links = FP (Verstoß wird erkannt obwohl keiner vorliegt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f2 = metrics.fbeta_score(actual, predicted, beta=2, pos_label=True)\n",
    "\n",
    "print(\"Accuracy: \" + str(round(accuracy, 4)))\n",
    "print(\"Precision: \" + str(round(precision, 4)))\n",
    "print(\"Recall: \" + str(round(recall, 4)))\n",
    "print(\"F2: \" + str(round(f2, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_original\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores:\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/FewShot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d', cbar=False, xticklabels=[\"Smell\", \"No Smell\"], yticklabels=[\"Smell\", \"No Smell\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f2 = metrics.fbeta_score(actual, predicted, beta=2, pos_label=True)\n",
    "\n",
    "print(\"Accuracy: \" + str(round(accuracy, 4)))\n",
    "print(\"Precision: \" + str(round(precision, 4)))\n",
    "print(\"Recall: \" + str(round(recall, 4)))\n",
    "print(\"F2: \" + str(round(f2, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_original\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores (Sum):\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/0ShotCoT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d', cbar=False, xticklabels=[\"Smell\", \"No Smell\"], yticklabels=[\"Smell\", \"No Smell\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f2 = metrics.fbeta_score(actual, predicted, beta=2, pos_label=True)\n",
    "\n",
    "print(\"Accuracy: \" + str(round(accuracy, 4)))\n",
    "print(\"Precision: \" + str(round(precision, 4)))\n",
    "print(\"Recall: \" + str(round(recall, 4)))\n",
    "print(\"F2: \" + str(round(f2, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_old\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores (Sum):\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/FewShotCoT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[True, False])\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d', cbar=False, xticklabels=[\"Smell\", \"No Smell\"], yticklabels=[\"Smell\", \"No Smell\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=True)\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=True)\n",
    "f2 = metrics.fbeta_score(actual, predicted, beta=2, pos_label=True)\n",
    "\n",
    "print(\"Accuracy: \" + str(round(accuracy, 4)))\n",
    "print(\"Precision: \" + str(round(precision, 4)))\n",
    "print(\"Recall: \" + str(round(recall, 4)))\n",
    "print(\"F2: \" + str(round(f2, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrue = evalDF_runs[\"correction_in_range\"].value_counts()[True]\n",
    "allFalse = evalDF_runs[\"correction_in_range\"].value_counts()[False]\n",
    "\n",
    "# correctedTickets = evalDF_runs[evalDF_runs[\"violation_actual\"] == \"TRUE\"]\n",
    "successRate = evalDF_runs[\"correction_in_range\"].value_counts(normalize=True)[True]\n",
    "\n",
    "print(\"# True: \" + str(allTrue))\n",
    "print(\"# False: \" + str(allFalse))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = np.array([])\n",
    "new_summary = np.array([])\n",
    "\n",
    "for index, row in evalDF.iterrows():\n",
    "\n",
    "    if row[\"violation_actual\"] == True:\n",
    "        reference = np.append(reference, row[\"summary_old\"])\n",
    "        new_summary = np.append(new_summary, row[\"summary_new\"])\n",
    "\n",
    "print(\"Average ROUGE scores (Sum):\")\n",
    "total_scores = computeROUGE(reference, new_summary)\n",
    "print(total_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShot_df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/0Shot/evaluatedSummarys.csv\")\n",
    "FewShot_df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/FewShot/evaluatedSummarys.csv\")\n",
    "ZeroShotCoT_df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/0ShotCoT/evaluatedSummarys.csv\")\n",
    "FewShotCoT_df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/FewShotCoT/evaluatedSummarys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShot_df[\"prompt\"] = \"0Shot\"\n",
    "FewShot_df[\"prompt\"] = \"FewShot\"\n",
    "ZeroShotCoT_df[\"prompt\"] = \"0ShotCoT\"\n",
    "FewShotCoT_df[\"prompt\"] = \"FewShotCoT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [ZeroShot_df, FewShot_df, ZeroShotCoT_df, FewShotCoT_df]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSummarys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_smells_df = result[result[\"violation_actual\"] == True]\n",
    "total_smells_0Run_df = total_smells_df[total_smells_df[\"reruns\"] == 0]\n",
    "total_smells_1Run_df = total_smells_df[total_smells_df[\"reruns\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_smells_0Run_df.to_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSmells_0Run.csv\", index=False)\n",
    "total_smells_1Run_df.to_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSmells_1Run.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create EvalCSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/summary/gpt-4-0125-preview/Consensus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"run\", \"ticket_uri\", \"output_uri\", \"summary_original\", \"preferred_summary\", \"prompt_type\", \"model_explanation\", \"my_choice\", \"match\"])\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    outputUri = directory + filename\n",
    "\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(outputUri) as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    summary_org = getOriginalSummary(result[\"input_data\"][\"ticket_uri\"])\n",
    "\n",
    "    new_row = {\n",
    "        'jira': result[\"input_data\"][\"jira\"], \n",
    "        'ticketId': result[\"input_data\"][\"id\"], \n",
    "        'evolution': result[\"input_data\"][\"evolution\"], \n",
    "        'run': result[\"reruns\"],\n",
    "        'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "        'output_uri': outputUri, \n",
    "        'summary_original': summary_org,\n",
    "        'preferred_summary': result[\"output\"][\"summary\"],\n",
    "        'prompt_type': result[\"output\"][\"prompt_type\"],\n",
    "        'model_explanation': result[\"output\"][\"explanation\"],\n",
    "        'my_choice': \" \", \n",
    "        'match': False\n",
    "        }\n",
    "\n",
    "    evalDF.loc[len(evalDF)]=new_row\n",
    "\n",
    "# Save data to csv\n",
    "evalDF.to_csv(directory + \"evaluatedSelection.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + \"evaluatedSelectionLabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row[\"prompt_type\"] == row[\"my_choice\"]:\n",
    "        df.at[index, \"match\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(directory + \"evaluatedSelectionLabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferencesByModel = df[\"prompt_type\"].value_counts()\n",
    "preferencesByMe = df[\"my_choice\"].value_counts()\n",
    "matches = df[\"match\"].value_counts()\n",
    "print(preferencesByModel)\n",
    "print(preferencesByMe)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate issue type labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSummarys.csv\")\n",
    "df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/Consensus/evaluatedSelectionLabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIssueTypeCode(collection, jira, type):\n",
    "    try:\n",
    "        return collection[jira][type]['code']\n",
    "    except:\n",
    "        return \"Not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('issueTypeMapping.json') as f:\n",
    "    mappedIssueTypes = json.load(f)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['jira'] + \"_\" + str(row['ticketId']) + \"_\" + str(row['evolution']) + \".json\"\n",
    "\n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "\n",
    "    issueType = getIssueTypeCode(mappedIssueTypes, row['jira'], ticket[\"IssueType\"])\n",
    "    df.at[index, \"issueType\"] = issueType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSummarysIssueTypeLabel.csv\", index=False)\n",
    "df.to_csv(\"./evaluation/summary/gpt-4-0125-preview/Consensus/evaluatedSelectionLabeledIssueType.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSummarysIssueTypeLabel.csv\")\n",
    "df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/Consensus/evaluatedSelectionLabeledIssueType.csv\")\n",
    "\n",
    "def getFirstWordPOS(summary):\n",
    "    doc = nlp(summary)\n",
    "    return doc[0].pos_\n",
    "\n",
    "def getNumberOfPOSPerType(df, issueTypes):\n",
    "    for issueType in issueTypes:\n",
    "        df_issueType = df[df[\"issueType\"] == issueType]\n",
    "        print(issueType)\n",
    "        print(df_issueType[\"firstWordPOS\"].value_counts())\n",
    "        print(\"   \")\n",
    "        print(\"###--------###\")\n",
    "        print(\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"firstWordPOS\"] = df[\"preferred_summary\"].apply(getFirstWordPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./evaluation/summary/gpt-4-0125-preview/Consensus/consensusLabeledSpacy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/totalSummarysLabeldSpacy.csv\")\n",
    "df = pd.read_csv(\"./evaluation/summary/gpt-4-0125-preview/Consensus/consensusLabeledSpacy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"firstWordPOS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issueTypes = df['issueType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroShot_df = df[df[\"prompt\"] == \"0Shot\"]\n",
    "fewShot_df = df[df[\"prompt\"] == \"FewShot\"]\n",
    "zeroShotCoT_df = df[df[\"prompt\"] == \"0ShotCoT\"]\n",
    "fewShotCoT_df = df[df[\"prompt\"] == \"FewShotCoT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroShot_df[\"firstWordPOS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getNumberOfPOSPerType(zeroShot_df, issueTypes)\n",
    "# getNumberOfPOSPerType(fewShot_df, issueTypes)\n",
    "# getNumberOfPOSPerType(zeroShotCoT_df, issueTypes)\n",
    "getNumberOfPOSPerType(fewShotCoT_df, issueTypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/update/gpt-4-0125-preview/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"reruns\", \"ticket_uri\", \"output_uri\", \"violation_actual\", \"violation_predicted\", \"change_actual\", \"change_predicted\", \"success\"])\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    outputUri = directory + filename\n",
    "\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(outputUri) as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    new_row = {\n",
    "        'jira': result[\"input_data\"][\"jira\"], \n",
    "        'ticketId': result[\"input_data\"][\"id\"], \n",
    "        'evolution': result[\"input_data\"][\"evolution\"],\n",
    "        'reruns': result[\"reruns\"],\n",
    "        'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "        'output_uri': outputUri, \n",
    "        'violation_actual': result[\"violation_actual\"], \n",
    "        'violation_predicted': result[\"output\"][\"violation_predicted\"],\n",
    "        'change_actual': result[\"reason\"],\n",
    "        'change_predicted': result[\"output\"][\"fields\"],\n",
    "        'success': None\n",
    "        }\n",
    "\n",
    "    evalDF.loc[len(evalDF)]=new_row    \n",
    "\n",
    "# Save data to csv\n",
    "evalDF.to_csv(directory + \"updateSummarys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"violation_actual\"])\n",
    "    predicted = np.append(predicted, row[\"violation_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision = metrics.precision_score(actual, predicted, pos_label=\"TRUE\")\n",
    "recall = metrics.recall_score(actual, predicted, pos_label=\"TRUE\")\n",
    "f05 = metrics.fbeta_score(actual, predicted, beta=0.5, pos_label=\"TRUE\")\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(actual, predicted, labels=[\"TRUE\", \"FALSE\"])\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"TRUE\", \"FALSE\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F05: \" + str(f05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"updateSummarys_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successRate = evalDF[\"success\"].value_counts(normalize=True)[True]\n",
    "print(\"Success rate: \" + str(successRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug Report Strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions ####\n",
    "##################\n",
    "\n",
    "def getOriginalDescription(uri):\n",
    "    \n",
    "    with open(uri) as f:\n",
    "        ticket = json.load(f)\n",
    "\n",
    "    desc = \"\"    \n",
    "    if ticket['Description'] is not None:\n",
    "        desc = ticket['Description']\n",
    "    \n",
    "    return desc    \n",
    "\n",
    "def createEvaluationCSV(directory):\n",
    "    evalDF = pd.DataFrame(columns=[\"jira\", \"ticketId\", \"evolution\", \"reruns\", \"ticket_uri\", \"output_uri\", \"desc_original\", \"desc_revised\", \"reason\", \"smell_degree_actual\", \"smell_degree_predicted\", \"equal_degree\"])\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        outputUri = directory + filename\n",
    "\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        with open(outputUri) as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        if result[\"output\"][\"smell_degree\"] == result[\"smell_actual\"]:\n",
    "            equal = \"TRUE\"\n",
    "        else:\n",
    "            equal = \"FALSE\"\n",
    "\n",
    "        desc_org = getOriginalDescription(result[\"input_data\"][\"ticket_uri\"])\n",
    "\n",
    "        new_row = {\n",
    "            'jira': result[\"input_data\"][\"jira\"], \n",
    "            'ticketId': result[\"input_data\"][\"id\"], \n",
    "            'evolution': result[\"input_data\"][\"evolution\"], \n",
    "            'reruns': result[\"reruns\"],\n",
    "            'ticket_uri': result[\"input_data\"][\"ticket_uri\"],\n",
    "            'output_uri': outputUri,\n",
    "            'desc_original': desc_org,\n",
    "            'desc_revised': result[\"output\"][\"description_new\"],\n",
    "            'reason': result[\"output\"][\"reason\"],\n",
    "            'smell_degree_actual': result[\"smell_actual\"], \n",
    "            'smell_degree_predicted': result[\"output\"][\"smell_degree\"],\n",
    "            'equal_degree': equal\n",
    "            }\n",
    "\n",
    "        evalDF.loc[len(evalDF)]=new_row\n",
    "\n",
    "    # Save data to csv\n",
    "    evalDF.to_csv(directory + \"evaluatedBugReportStructure.csv\", index=False)\n",
    "\n",
    "def getBinaryConfusionMetrics(cm, class_index):\n",
    "    TP = cm[class_index, class_index]\n",
    "    FP = cm[:, class_index].sum() - TP\n",
    "    FN = cm[class_index, :].sum() - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    binary_cm = np.array([[TP, FN], [FP, TN]])\n",
    "\n",
    "    accuracy = (TP + TN) / cm.sum()\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    f2_score = (5 * precision * recall) / (4 * precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    metrics = f\"Accuracy: {round(accuracy, 4)}, Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, F2-Score: {round(f2_score, 4)}\"\n",
    "    \n",
    "    return binary_cm, metrics\n",
    "\n",
    "def plotConfusionMatrix(cm, label):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.set(font_scale=5.0)\n",
    "    plt.title(f'2x2 Confusion Matrix for Degree {label}')\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[label, f'not {label}'], yticklabels=[label, f'not {label}'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/bugreportStructure/gpt-4-0125-preview/0Shot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedBugReportStructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"smell_degree_actual\"])\n",
    "    predicted = np.append(predicted, row[\"smell_degree_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.69      1.00      0.82        18\n",
      "         2.0       0.58      0.73      0.65        30\n",
      "         3.0       1.00      0.56      0.71        36\n",
      "\n",
      "    accuracy                           0.71        84\n",
      "   macro avg       0.76      0.76      0.73        84\n",
      "weighted avg       0.78      0.71      0.71        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.set(font_scale=4.0)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[1,2,3], yticklabels=[1,2,3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7143\n",
      "Precision (Macro-average): 0.7571\n",
      "Precision (Micro-average): 0.7143\n",
      "Precision (Weighted-average): 0.7837\n",
      "Recall (Macro-average): 0.763\n",
      "Recall (Micro-average): 0.7143\n",
      "Recall (Weighted-average): 0.7143\n",
      "F2 Score (Macro-average): 0.7414\n",
      "F2 Score (Micro-average): 0.7143\n",
      "F2 Score (Weighted-average): 0.7068\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision_macro = metrics.precision_score(actual, predicted, average='macro')\n",
    "precision_micro = metrics.precision_score(actual, predicted, average='micro')\n",
    "precision_weighted = metrics.precision_score(actual, predicted, average='weighted')\n",
    "recall_macro = metrics.recall_score(actual, predicted, average='macro')\n",
    "recall_micro = metrics.recall_score(actual, predicted, average='micro')\n",
    "recall_weighted = metrics.recall_score(actual, predicted, average='weighted')\n",
    "f2_macro = metrics.fbeta_score(actual, predicted, beta=2, average='macro')\n",
    "f2_micro = metrics.fbeta_score(actual, predicted, beta=2, average='micro')\n",
    "f2_weighted = metrics.fbeta_score(actual, predicted, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Precision (Macro-average): {round(precision_macro, 4)}\")\n",
    "print(f\"Precision (Micro-average): {round(precision_micro, 4)}\")\n",
    "print(f\"Precision (Weighted-average): {round(precision_weighted, 4)}\")\n",
    "print(f\"Recall (Macro-average): {round(recall_macro, 4)}\")\n",
    "print(f\"Recall (Micro-average): {round(recall_micro, 4)}\")\n",
    "print(f\"Recall (Weighted-average): {round(recall_weighted, 4)}\")\n",
    "print(f\"F2 Score (Macro-average): {round(f2_macro, 4)}\")\n",
    "print(f\"F2 Score (Micro-average): {round(f2_micro, 4)}\")\n",
    "print(f\"F2 Score (Weighted-average): {round(f2_weighted, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_name in enumerate(['1', '2', '3']):\n",
    "    binary_cm, metrics = getBinaryConfusionMetrics(confusion_matrix, i)\n",
    "    plotConfusionMatrix(binary_cm, class_name)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualCounts = evalDF_runs[\"smell_degree_actual\"].value_counts()\n",
    "predictedCounts = evalDF_runs[\"smell_degree_predicted\"].value_counts()\n",
    "successRate = evalDF_runs[\"equal_degree\"].value_counts(normalize=True)[True]\n",
    "true_entries = evalDF_runs[\"equal_degree\"].value_counts()[True]\n",
    "false_entries = evalDF_runs[\"equal_degree\"].value_counts()[False]\n",
    "\n",
    "print(actualCounts)\n",
    "print(predictedCounts)\n",
    "print(\"# of True: \" + str(true_entries))\n",
    "print(\"# of False: \" + str(false_entries))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/bugreportStructure/gpt-4-0125-preview/0ShotCoT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedBugReportStructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"smell_degree_actual\"])\n",
    "    predicted = np.append(predicted, row[\"smell_degree_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.set(font_scale=4.0)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[1,2,3], yticklabels=[1,2,3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision_macro = metrics.precision_score(actual, predicted, average='macro')\n",
    "precision_micro = metrics.precision_score(actual, predicted, average='micro')\n",
    "precision_weighted = metrics.precision_score(actual, predicted, average='weighted')\n",
    "recall_macro = metrics.recall_score(actual, predicted, average='macro')\n",
    "recall_micro = metrics.recall_score(actual, predicted, average='micro')\n",
    "recall_weighted = metrics.recall_score(actual, predicted, average='weighted')\n",
    "f2_macro = metrics.fbeta_score(actual, predicted, beta=2, average='macro')\n",
    "f2_micro = metrics.fbeta_score(actual, predicted, beta=2, average='micro')\n",
    "f2_weighted = metrics.fbeta_score(actual, predicted, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Precision (Macro-average): {round(precision_macro, 4)}\")\n",
    "print(f\"Precision (Micro-average): {round(precision_micro, 4)}\")\n",
    "print(f\"Precision (Weighted-average): {round(precision_weighted, 4)}\")\n",
    "print(f\"Recall (Macro-average): {round(recall_macro, 4)}\")\n",
    "print(f\"Recall (Micro-average): {round(recall_micro, 4)}\")\n",
    "print(f\"Recall (Weighted-average): {round(recall_weighted, 4)}\")\n",
    "print(f\"F2 Score (Macro-average): {round(f2_macro, 4)}\")\n",
    "print(f\"F2 Score (Micro-average): {round(f2_micro, 4)}\")\n",
    "print(f\"F2 Score (Weighted-average): {round(f2_weighted, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_name in enumerate(['1', '2', '3']):\n",
    "    binary_cm, metrics = getBinaryConfusionMetrics(confusion_matrix, i)\n",
    "    plotConfusionMatrix(binary_cm, class_name)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualCounts = evalDF_runs[\"smell_degree_actual\"].value_counts()\n",
    "predictedCounts = evalDF_runs[\"smell_degree_predicted\"].value_counts()\n",
    "successRate = evalDF_runs[\"equal_degree\"].value_counts(normalize=True)[True]\n",
    "true_entries = evalDF_runs[\"equal_degree\"].value_counts()[True]\n",
    "false_entries = evalDF_runs[\"equal_degree\"].value_counts()[False]\n",
    "\n",
    "print(actualCounts)\n",
    "print(predictedCounts)\n",
    "print(\"# of True: \" + str(true_entries))\n",
    "print(\"# of False: \" + str(false_entries))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/bugreportStructure/gpt-4-0125-preview/FewShot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedBugReportStructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"smell_degree_actual\"])\n",
    "    predicted = np.append(predicted, row[\"smell_degree_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.set(font_scale=4.0)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[1,2,3], yticklabels=[1,2,3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision_macro = metrics.precision_score(actual, predicted, average='macro')\n",
    "precision_micro = metrics.precision_score(actual, predicted, average='micro')\n",
    "precision_weighted = metrics.precision_score(actual, predicted, average='weighted')\n",
    "recall_macro = metrics.recall_score(actual, predicted, average='macro')\n",
    "recall_micro = metrics.recall_score(actual, predicted, average='micro')\n",
    "recall_weighted = metrics.recall_score(actual, predicted, average='weighted')\n",
    "f2_macro = metrics.fbeta_score(actual, predicted, beta=2, average='macro')\n",
    "f2_micro = metrics.fbeta_score(actual, predicted, beta=2, average='micro')\n",
    "f2_weighted = metrics.fbeta_score(actual, predicted, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Precision (Macro-average): {round(precision_macro, 4)}\")\n",
    "print(f\"Precision (Micro-average): {round(precision_micro, 4)}\")\n",
    "print(f\"Precision (Weighted-average): {round(precision_weighted, 4)}\")\n",
    "print(f\"Recall (Macro-average): {round(recall_macro, 4)}\")\n",
    "print(f\"Recall (Micro-average): {round(recall_micro, 4)}\")\n",
    "print(f\"Recall (Weighted-average): {round(recall_weighted, 4)}\")\n",
    "print(f\"F2 Score (Macro-average): {round(f2_macro, 4)}\")\n",
    "print(f\"F2 Score (Micro-average): {round(f2_micro, 4)}\")\n",
    "print(f\"F2 Score (Weighted-average): {round(f2_weighted, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_name in enumerate(['1', '2', '3']):\n",
    "    binary_cm, metrics = getBinaryConfusionMetrics(confusion_matrix, i)\n",
    "    plotConfusionMatrix(binary_cm, class_name)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualCounts = evalDF_runs[\"smell_degree_actual\"].value_counts()\n",
    "predictedCounts = evalDF_runs[\"smell_degree_predicted\"].value_counts()\n",
    "successRate = evalDF_runs[\"equal_degree\"].value_counts(normalize=True)[True]\n",
    "true_entries = evalDF_runs[\"equal_degree\"].value_counts()[True]\n",
    "false_entries = evalDF_runs[\"equal_degree\"].value_counts()[False]\n",
    "\n",
    "print(actualCounts)\n",
    "print(predictedCounts)\n",
    "print(\"# of True: \" + str(true_entries))\n",
    "print(\"# of False: \" + str(false_entries))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./evaluation/bugreportStructure/gpt-4-0125-preview/FewShotCoT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createEvaluationCSV(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalDF = pd.read_csv(directory + \"evaluatedBugReportStructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([])\n",
    "predicted = np.array([])\n",
    "\n",
    "evalDF_runs = evalDF[evalDF[\"reruns\"] <= 1]\n",
    "\n",
    "for index, row in evalDF_runs.iterrows():\n",
    "\n",
    "    actual = np.append(actual, row[\"smell_degree_actual\"])\n",
    "    predicted = np.append(predicted, row[\"smell_degree_predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.set(font_scale=4.0)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[1,2,3], yticklabels=[1,2,3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(actual, predicted)\n",
    "precision_macro = metrics.precision_score(actual, predicted, average='macro')\n",
    "precision_micro = metrics.precision_score(actual, predicted, average='micro')\n",
    "precision_weighted = metrics.precision_score(actual, predicted, average='weighted')\n",
    "recall_macro = metrics.recall_score(actual, predicted, average='macro')\n",
    "recall_micro = metrics.recall_score(actual, predicted, average='micro')\n",
    "recall_weighted = metrics.recall_score(actual, predicted, average='weighted')\n",
    "f2_macro = metrics.fbeta_score(actual, predicted, beta=2, average='macro')\n",
    "f2_micro = metrics.fbeta_score(actual, predicted, beta=2, average='micro')\n",
    "f2_weighted = metrics.fbeta_score(actual, predicted, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Precision (Macro-average): {round(precision_macro, 4)}\")\n",
    "print(f\"Precision (Micro-average): {round(precision_micro, 4)}\")\n",
    "print(f\"Precision (Weighted-average): {round(precision_weighted, 4)}\")\n",
    "print(f\"Recall (Macro-average): {round(recall_macro, 4)}\")\n",
    "print(f\"Recall (Micro-average): {round(recall_micro, 4)}\")\n",
    "print(f\"Recall (Weighted-average): {round(recall_weighted, 4)}\")\n",
    "print(f\"F2 Score (Macro-average): {round(f2_macro, 4)}\")\n",
    "print(f\"F2 Score (Micro-average): {round(f2_micro, 4)}\")\n",
    "print(f\"F2 Score (Weighted-average): {round(f2_weighted, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_name in enumerate(['1', '2', '3']):\n",
    "    binary_cm, metrics = getBinaryConfusionMetrics(confusion_matrix, i)\n",
    "    plotConfusionMatrix(binary_cm, class_name)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualCounts = evalDF_runs[\"smell_degree_actual\"].value_counts()\n",
    "predictedCounts = evalDF_runs[\"smell_degree_predicted\"].value_counts()\n",
    "successRate = evalDF_runs[\"equal_degree\"].value_counts(normalize=True)[True]\n",
    "true_entries = evalDF_runs[\"equal_degree\"].value_counts()[True]\n",
    "false_entries = evalDF_runs[\"equal_degree\"].value_counts()[False]\n",
    "\n",
    "print(actualCounts)\n",
    "print(predictedCounts)\n",
    "print(\"# of True: \" + str(true_entries))\n",
    "print(\"# of False: \" + str(false_entries))\n",
    "print(\"Success rate: \" + str(round(successRate, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShot_df = pd.read_csv(\"./evaluation/bugreportStructure/gpt-4-0125-preview/0Shot/evaluatedBugReportStructure.csv\")\n",
    "FewShot_df = pd.read_csv(\"./evaluation/bugreportStructure/gpt-4-0125-preview/FewShot/evaluatedBugReportStructure.csv\")\n",
    "ZeroShotCoT_df = pd.read_csv(\"./evaluation/bugreportStructure/gpt-4-0125-preview/0ShotCoT/evaluatedBugReportStructure.csv\")\n",
    "FewShotCoT_df = pd.read_csv(\"./evaluation/bugreportStructure/gpt-4-0125-preview/FewShotCoT/evaluatedBugReportStructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZeroShot_df[\"prompt\"] = \"0Shot\"\n",
    "FewShot_df[\"prompt\"] = \"FewShot\"\n",
    "ZeroShotCoT_df[\"prompt\"] = \"0ShotCoT\"\n",
    "FewShotCoT_df[\"prompt\"] = \"FewShotCoT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [ZeroShot_df, FewShot_df, ZeroShotCoT_df, FewShotCoT_df]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"./evaluation/bugreportStructure/gpt-4-0125-preview/totalBugReportStructure.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
