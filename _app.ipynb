{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import helper\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(output, prompt_type, model, init, best_practice, format_prompt, output_parser, ticketUri, ticket, reruns=0, evalMode=False):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'init_uri': init,\n",
    "        'best_practice_uri': best_practice,\n",
    "        'formatter_uri': format_prompt}\n",
    "    result['output_parser'] = output_parser\n",
    "    result['input_data'] = {\n",
    "        'ticket_uri': ticketUri,\n",
    "        'jira': ticket['Jira'],\n",
    "        'id': ticket['IssueId'],\n",
    "        'evolution': ticket['EvoId']}\n",
    "    result['reruns'] = reruns\n",
    "    result['output'] = output\n",
    "\n",
    "    helper.annotateResult(result, ticket, prompt_type, evalMode)\n",
    "\n",
    "    if evalMode:\n",
    "        with open('evaluation/'+prompt_type+'/output_'+date+'_'+time+'.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "        print('Evaluation mode: result saved in evaluation folder')\n",
    "\n",
    "    with open('results/'+prompt_type+'/output_'+date+'_'+time+'.json', 'w') as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    api_key=open('api.txt', 'r').read(),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDetectionOfSummaryLength(init_prompt_name, bestPractice_prompt_name, format_prompt_name, ticketUri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "    bestPractice_prompt = load_prompt('prompts/summary/'+bestPractice_prompt_name).format(min=minChars, max=maxChars)\n",
    "    format_prompt = load_prompt('prompts/summary/'+format_prompt_name)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = chain2.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(output, 'summary', model_name, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "testDataset = annotatedDataset.sample(2)\n",
    "testDataset['violation_predicted'] = None\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "\n",
    "    # Set preditedc value as new entry in the dataset\n",
    "    # row['violation_predicted'] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runDetectionOfSummaryLength('initPrompt_V1.3.0.json', 'summaryLengthPrompt_V2.3.0.json', 'summaryLengthFormatPrompt_V1.2.1.json', json_sample. ticketUri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"suggestion_sample.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'descriptionCompletenessPrompt_V1.0.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/description/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'descriptionCompletenessFormatPrompt_V1.0.0.json'\n",
    "format_prompt = load_prompt('prompts/description/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"revised_ticket\": chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'description', model_name, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe try Few-Shot here for better results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"suggestion_sample.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'updatePrompt_V1.16.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/update/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'updateFormatPrompt_V1.3.0.json'\n",
    "format_prompt = load_prompt('prompts/update/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"revised_ticket\": chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'update', model_name, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_toxic.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'toxicSpeechPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/toxicSpeech/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'toxicSpeechFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/toxicSpeech/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"revised_ticket\": chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Content Moderator\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'toxicSpeech', model_name, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"desc_struc_last_row.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStrucutre_prompt_name = 'bugReportStructurePrompt_V2.3.0.json'\n",
    "bugReportStrucutre_prompt = load_prompt('prompts/bugReportStructure/'+bugReportStrucutre_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = bugReportStrucutre_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"bug_report\":json_sample})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why data from comments field is taken into account here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'bugReportStructure', model_name, '-', bugReportStrucutre_prompt_name, '-', 'StrOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_german.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'internationalizationPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/internationalization/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'internationalizationFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/internationalization/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"revised_ticket\":chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'internationalization', model_name, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
