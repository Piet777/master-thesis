{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(output, prompt_type, model, init, best_practice, output_parser):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'init_uri': init,\n",
    "        'best_practice_uri': best_practice}\n",
    "    result['output_parser'] = output_parser \n",
    "    result['output'] = output\n",
    "\n",
    "    with open('results/'+prompt_type+'/output_'+date+'_'+time+'.json', 'w') as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StringToJSON(string):\n",
    "    result = string\n",
    "\n",
    "    try:\n",
    "        if ((string[0] != \"{\") and (string[0] != \"[\")):\n",
    "            string = \"{\"+string+\"}\"\n",
    "\n",
    "        result = json.loads(string)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"JSON formatting went wrong. The result returns as a string.\")\n",
    "        \n",
    "    finally:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    api_key=open('api.txt', 'r').read(),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/json/last_row/last_row.json') as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# class ListItem(BaseModel):\n",
    "#     \"\"\"Item of a list\"\"\"\n",
    "\n",
    "#     id: int = Field(description=\"The item identifier\")\n",
    "#     content: str = Field(description=\"The item content\")\n",
    "\n",
    "#     @validator(\"id\")\n",
    "#     def validate_id(cls, field):\n",
    "#         if not field:\n",
    "#             raise ValueError(\"Item ID cannot be empty!\")\n",
    "#         return field\n",
    "    \n",
    "#     @validator(\"content\")\n",
    "#     def validate_content(cls, field):\n",
    "#         if not field:\n",
    "#             raise ValueError(\"Item content cannot be empty!\")\n",
    "#         return field\n",
    "    \n",
    "# parser = PydanticOutputParser(pydantic_object=ListItem)\n",
    "# ...\n",
    "# bestPractice_prompt = load_prompt('prompts/update/'+bestPractice_prompt_name).format(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'summaryLengthPrompt_V2.2.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/summary/'+bestPractice_prompt_name).format(min=39, max=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"SourceTree crashes upon opening, despite multiple reinstallations\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be between 39 and 70\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'summary', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe usefull for later\n",
    "# def run(model_name, init_prompt_name, sumLen_prompt_name):\n",
    "#     model = ChatOpenAI(\n",
    "#         model=model_name,\n",
    "#         api_key=open('api.txt', 'r').read(),\n",
    "#         temperature=0\n",
    "#     )\n",
    "#     with open('./data/json/last_row.json') as f:\n",
    "#         json_sample = json.load(f)\n",
    "\n",
    "#     init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "#     sumLen_prompt = load_prompt('prompts/summary/'+sumLen_prompt_name).format(min= 6, max= 10)\n",
    "    \n",
    "#     chain = init_prompt | model | StrOutputParser()\n",
    "\n",
    "#     output = chain.invoke({\"best_practice\": sumLen_prompt,\"ticket\": json_sample})\n",
    "\n",
    "#     save_result(output, 'summary', model_name, init_prompt_name, sumLen_prompt_name)  \n",
    "\n",
    "# run('gpt-4', 'initPrompt_V1.0.0.json', 'summaryLengthPrompt_V1.0.1.json')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'descriptionLengthPrompt_V1.0.2.json'\n",
    "bestPractice_prompt = load_prompt('prompts/description/'+bestPractice_prompt_name).format(min= 29, max= 111)\n",
    "\n",
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Source tree crashes every time I open it. I have uninstalled and reinstalled it multiple times but the issue persists. This issue is occurring in version 3.4.4. I need assistance in resolving this issue as it is hindering my work.'\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'description', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'updatePrompt_V1.12.2.json'\n",
    "bestPractice_prompt = load_prompt('prompts/update/'+bestPractice_prompt_name).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'update', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/json/last_row/last_row_toxic.json') as f:\n",
    "    toxic_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'toxicSpeechPrompt_V1.2.4.json'\n",
    "bestPractice_prompt = load_prompt('prompts/toxicSpeech/'+bestPractice_prompt_name).format()\n",
    "\n",
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Content Moderator\", \"best_practice\":bestPractice_prompt, \"ticket\":toxic_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'toxicSpeech', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStrucutre_prompt_name = 'bugReportStructurePrompt_V1.0.1.json'\n",
    "bugReportStrucutre_prompt = load_prompt('prompts/bugReportStructure/'+bugReportStrucutre_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = bugReportStrucutre_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"bug_Report\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'bugReportStructure', model_name, '-', bugReportStrucutre_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/json/last_row/last_row_german.json') as f:\n",
    "    german_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'internationalizationPrompt_V1.2.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/internationalization/'+bestPractice_prompt_name).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":german_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Summary\": \"Sourcetree crashes\",\n",
      "\"Description\": \"The Sourcetree crashes every time I open it, I have uninstalled and installed several times\"}\n"
     ]
    }
   ],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'internationalization', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
