{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(output, prompt_type, model, init, best_practice, output_parser):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'init_uri': init,\n",
    "        'best_practice_uri': best_practice}\n",
    "    result['output_parser'] = output_parser \n",
    "    result['output'] = output\n",
    "\n",
    "    with open('results/'+prompt_type+'/output_'+date+'_'+time+'.json', 'w') as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StringToJSON(string):\n",
    "    result = string\n",
    "\n",
    "    try:\n",
    "        if ((string[0] != \"{\") and (string[0] != \"[\")):\n",
    "            string = \"{\"+string+\"}\"\n",
    "\n",
    "        result = json.loads(string)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"JSON formatting went wrong. The result returns as a string.\")\n",
    "        \n",
    "    finally:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=model_name,\n",
    "    api_key=open('api.txt', 'r').read(),\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"desc_struc_last_row.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# class ListItem(BaseModel):\n",
    "#     \"\"\"Item of a list\"\"\"\n",
    "\n",
    "#     id: int = Field(description=\"The item identifier\")\n",
    "#     content: str = Field(description=\"The item content\")\n",
    "\n",
    "#     @validator(\"id\")\n",
    "#     def validate_id(cls, field):\n",
    "#         if not field:\n",
    "#             raise ValueError(\"Item ID cannot be empty!\")\n",
    "#         return field\n",
    "    \n",
    "#     @validator(\"content\")\n",
    "#     def validate_content(cls, field):\n",
    "#         if not field:\n",
    "#             raise ValueError(\"Item content cannot be empty!\")\n",
    "#         return field\n",
    "    \n",
    "# parser = PydanticOutputParser(pydantic_object=ListItem)\n",
    "# ...\n",
    "# bestPractice_prompt = load_prompt('prompts/update/'+bestPractice_prompt_name).format(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'summaryLengthPrompt_V2.2.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/summary/'+bestPractice_prompt_name).format(min=39, max=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Single page display of project components and their issue statuses\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be between 39 and 70\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'summary', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # maybe usefull for later\n",
    "# def run(model_name, init_prompt_name, sumLen_prompt_name):\n",
    "#     model = ChatOpenAI(\n",
    "#         model=model_name,\n",
    "#         api_key=open('api.txt', 'r').read(),\n",
    "#         temperature=0\n",
    "#     )\n",
    "#     with open('./data/json/last_row.json') as f:\n",
    "#         json_sample = json.load(f)\n",
    "\n",
    "#     init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "#     sumLen_prompt = load_prompt('prompts/summary/'+sumLen_prompt_name).format(min= 6, max= 10)\n",
    "    \n",
    "#     chain = init_prompt | model | StrOutputParser()\n",
    "\n",
    "#     output = chain.invoke({\"best_practice\": sumLen_prompt,\"ticket\": json_sample})\n",
    "\n",
    "#     save_result(output, 'summary', model_name, init_prompt_name, sumLen_prompt_name)  \n",
    "\n",
    "# run('gpt-4', 'initPrompt_V1.0.0.json', 'summaryLengthPrompt_V1.0.1.json')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Length\n",
    "Does this make any sense (no additional data are in the productive available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'descriptionLengthPrompt_V2.7.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/description/'+bestPractice_prompt_name).format(min= 210, max= 850)\n",
    "\n",
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The description of the ticket is: \"A single page displaying all the different components for a project, and the status of different issues for the component. Could work with either open bugs per component, or planned features per component. (For all versions or for a specific version) See http://www7b.software.ibm.com/webapp/wsdd/wasServlet3?client=form10&SID=1013344492744 for how it might possibly look (I like the graphical status indicators)\". \\n\\nThis description is too short as per the best practice of having between 210 and 850 characters. \\n\\nRecommended description: \"The issue pertains to the development of a single page that displays all the different components of a project, along with the status of various issues related to each component. The page should be designed to work with either open bugs per component or planned features per component. This functionality should be applicable for all versions of the project or for a specific version, as required. For a visual representation of how this might look, refer to the following link: http://www7b.software.ibm.com/webapp/wsdd/wasServlet3?client=form10&SID=1013344492744. The graphical status indicators on this page are particularly appealing and could be incorporated into our design.\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'description', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'updatePrompt_V1.12.2.json'\n",
    "bestPractice_prompt = load_prompt('prompts/update/'+bestPractice_prompt_name).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'update', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/json/last_row/last_row_toxic.json') as f:\n",
    "    toxic_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'toxicSpeechPrompt_V1.2.4.json'\n",
    "bestPractice_prompt = load_prompt('prompts/toxicSpeech/'+bestPractice_prompt_name).format()\n",
    "\n",
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Content Moderator\", \"best_practice\":bestPractice_prompt, \"ticket\":toxic_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'toxicSpeech', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStrucutre_prompt_name = 'bugReportStructurePrompt_V2.1.0.json'\n",
    "bugReportStrucutre_prompt = load_prompt('prompts/bugReportStructure/'+bugReportStrucutre_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = bugReportStrucutre_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"bug_report\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'bugReportStructure', model_name, '-', bugReportStrucutre_prompt_name, 'StrOutputParser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/json/last_row/last_row_german.json') as f:\n",
    "    german_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.1.2.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'internationalizationPrompt_V1.2.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/internationalization/'+bestPractice_prompt_name).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = init_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":german_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Summary\": \"Sourcetree crashes\",\n",
      "\"Description\": \"The Sourcetree crashes every time I open it, I have uninstalled and installed several times\"}\n"
     ]
    }
   ],
   "source": [
    "output_json = StringToJSON(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output_json, 'internationalization', model_name, init_prompt_name, bestPractice_prompt_name, 'StrOutputParser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
