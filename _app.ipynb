{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import helper\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(algorithm, output, prompt_type, model, init, best_practice, format_prompt, output_parser, ticketUri, ticket, reruns=0, evalMode=False):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'init_uri': init,\n",
    "        'best_practice_uri': best_practice,\n",
    "        'formatter_uri': format_prompt}\n",
    "    result['output_parser'] = output_parser\n",
    "    result['input_data'] = {\n",
    "        'ticket_uri': ticketUri,\n",
    "        'jira': ticket['Jira'],\n",
    "        'id': ticket['IssueId'],\n",
    "        'evolution': ticket['EvoId']}\n",
    "    result['reruns'] = reruns\n",
    "    result['output'] = output\n",
    "\n",
    "    helper.annotateResult(result, ticket, prompt_type)\n",
    "\n",
    "    output_name = 'output_'+date+'_'+time+'.json'\n",
    "\n",
    "    if evalMode:\n",
    "        with open('evaluation/' + prompt_type + '/' + model + '/' + algorithm + \"/\" + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "        print('Evaluation mode: result saved in evaluation folder')\n",
    "    else:\n",
    "        with open('results/' + prompt_type + '/' + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'gpt-4-0125-preview'\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=MODELNAME,\n",
    "    api_key=open('api.txt', 'r').read(),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Hier wird das Ticket noch über der init_prompt eingespielt\n",
    "# INITPROMPT = 'initPrompt_V1.3.0.json'\n",
    "\n",
    "# Hier wird das Ticket über der best_practice_prompt eingespielt\n",
    "INITPROMPT = 'initPrompt_V2.1.0.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARYPROMPT = 'summaryLengthPrompt_V2.4.0.json'\n",
    "SUMMARYFORMATPROMPT = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfSummaryLength(ticketUri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/summary/' + SUMMARYPROMPT).format(min=minChars, max=maxChars)\n",
    "    format_prompt = load_prompt('prompts/summary/' + SUMMARYFORMATPROMPT)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result('0Shot', output, 'summary', MODELNAME, INITPROMPT, SUMMARYPROMPT, SUMMARYFORMATPROMPT, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength(ticketUri, ticket, 1, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARYPROMPT = 'summaryLengthPrompt_FewShot_V2.3.0.json'\n",
    "SUMMARYFORMATPROMPT = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfSummaryLength(ticketUri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/summary/' + SUMMARYPROMPT).format(min=minChars, max=maxChars, ticket=ticket)\n",
    "    format_prompt = load_prompt('prompts/summary/' + SUMMARYFORMATPROMPT)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "    \n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result('FewShot', output, 'summary', MODELNAME, INITPROMPT, SUMMARYPROMPT, SUMMARYFORMATPROMPT, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength(ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARYPROMPT = 'summaryLengthPrompt_0ShotCoT_V1.0.0.json'\n",
    "SUMMARYFORMATPROMPT = 'summaryLengthFormatPrompt_V1.4.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfSummaryLength(ticketUri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/summary/' + SUMMARYPROMPT).format(min=minChars, max=maxChars)\n",
    "    format_prompt = load_prompt('prompts/summary/' + SUMMARYFORMATPROMPT)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result('0ShotCoT', output, 'summary', MODELNAME, INITPROMPT, SUMMARYPROMPT, SUMMARYFORMATPROMPT, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength(ticketUri, ticket, 2, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbitrary Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDetectionOfArbitraryStructure(init_prompt_name, bestPractice_prompt_name, format_prompt_name, ticketUri, ticket, issue_type, desired_structure, structure_desc, reruns= 0, evalMode=False):\n",
    "    ### Set Prompts ### \n",
    "    init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "    bestPractice_prompt = load_prompt('prompts/arbitraryStructure/'+bestPractice_prompt_name).format(issue_type=issue_type, structure_desc=structure_desc, structure=desired_structure)\n",
    "    format_prompt = load_prompt('prompts/arbitraryStructure/'+format_prompt_name)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = chain2.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(output, 'summary', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_type = \"User Story\"\n",
    "desired_structure = \"As a <Role>, I want/want to/need/can/would <Task>, so that <Goal>\"\n",
    "structure_desc = \"\"\"Role: abstract behavior of actors in the system context; describes who uses the system.\n",
    "Task: specific things that must be done to achieve goals; solution or function of the problem.\n",
    "Goal: a condition or a circumstance desired by stakeholders or actors; describes the problem domain or the impact of solving the problem.\"\"\"\n",
    "# structure_desc = \"\"\"Role: abstract behavior of actors in the system context; describes who uses the system.\n",
    "# Task: specific things that must be done to achieve goals; solution or function of the problem.\n",
    "# Goal: a condition or a circumstance desired by stakeholders or actors; describes the problem domain or the impact of solving the problem.\n",
    "# Capability: the ability of actors to achieve goals based on certain conditions and events.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/arbitraryStructure/arbitraryStructureDataset.csv')\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/arbitraryStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "\n",
    "    with open(ticketUri) as f:\n",
    "        json_sample = json.load(f)\n",
    "    \n",
    "    try:\n",
    "        runDetectionOfArbitraryStructure('initPrompt_V1.3.0.json', 'arbitraryStructurePrompt_V1.0.0.json', 'arbitraryStructureFormatPrompt_V1.0.0.json', ticketUri, json_sample, issue_type, desired_structure, structure_desc)\n",
    "    except:\n",
    "        print(\"Error with ticket: \" + ticketUri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATEPROMPT = 'updatePrompt_V1.26.0.json'\n",
    "UPDATEFORMATPROMPT = 'updateFormatPrompt_V1.11.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfOutdatedFields(ticket, ticketUri, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/update/' + UPDATEPROMPT).format()\n",
    "    format_prompt = load_prompt('prompts/update/' + UPDATEFORMATPROMPT)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    # ### Save result ###\n",
    "    save_result(output, 'update', MODELNAME, INITPROMPT, UPDATEPROMPT, UPDATEFORMATPROMPT, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/update/updateDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/update/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfOutdatedFields(ticket, ticketUri, 1, True)\n",
    "        print(\"     \")\n",
    "    except Exception as e:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        print(e)\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_toxic.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'toxicSpeechPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/toxicSpeech/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'toxicSpeechFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/toxicSpeech/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"revised_ticket\": chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Content Moderator\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'toxicSpeech', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUGREPORTPROMPT = 'bugReportStructurePrompt_V2.3.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"Jira_1388024_0.json\"\n",
    "with open('./data/bugreportStructure/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportStrucutre_prompt = load_prompt('prompts/bugReportStructure/'+BUGREPORTPROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = bugReportStrucutre_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain.ainvoke({\"bug_report\":json_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'bugReportStructure', MODELNAME, '-', BUGREPORTPROMPT, '-', 'StrOutputParser', sample_name, json_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_german.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'internationalizationPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/internationalization/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'internationalizationFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/internationalization/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"revised_ticket\":chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'internationalization', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
