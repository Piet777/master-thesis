{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import helper\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(algorithm, output, prompt_type, model, init, best_practice, format_prompt, output_parser, ticketUri, ticket, reruns=0, evalMode=False):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'init_uri': init,\n",
    "        'best_practice_uri': best_practice,\n",
    "        'formatter_uri': format_prompt}\n",
    "    result['output_parser'] = output_parser\n",
    "    result['input_data'] = {\n",
    "        'ticket_uri': ticketUri,\n",
    "        'jira': ticket['Jira'],\n",
    "        'id': ticket['IssueId'],\n",
    "        'evolution': ticket['EvoId']}\n",
    "    result['reruns'] = reruns\n",
    "    result['output'] = output\n",
    "\n",
    "    helper.annotateResult(result, ticket, prompt_type)\n",
    "\n",
    "    output_name = 'output_'+date+'_'+time+'.json'\n",
    "    \n",
    "    if evalMode:\n",
    "        with open('evaluation/' + prompt_type + '/' + model + '/' + algorithm + \"/\" + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "        print('Evaluation mode: result saved in evaluation folder')\n",
    "    else:\n",
    "        with open('results/' + prompt_type + '/' + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "def prepareConsensusTask(sample):\n",
    "    answers = \"\"\n",
    "    i = 1\n",
    "    for index, row in sample.iterrows():\n",
    "        answers += f\"Summary {i} from prompt type '{row['prompt']}': {row['summary_new']}\\n\"\n",
    "        i += 1\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def saveSelection(algorithm, output, prompt_type, model, consensus_prompt, format_prompt, output_parser, ticketUri, ticket, reruns=0, evalMode=False):\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d\")\n",
    "    time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    result = {}\n",
    "    result['model'] = model\n",
    "    result['creation_timestamp'] = date + ' ' + time\n",
    "    result['prompts'] = {\n",
    "        'consensus_uri': consensus_prompt,\n",
    "        'formatter_uri': format_prompt}\n",
    "    result['output_parser'] = output_parser\n",
    "    result['input_data'] = {\n",
    "        'ticket_uri': ticketUri,\n",
    "        'jira': ticket['Jira'],\n",
    "        'id': ticket['IssueId'],\n",
    "        'evolution': ticket['EvoId']}\n",
    "    result['reruns'] = reruns\n",
    "    result['output'] = output\n",
    "\n",
    "    output_name = 'output_'+date+'_'+time+'.json'\n",
    "\n",
    "    if evalMode:\n",
    "        with open('evaluation/' + prompt_type + '/' + model + '/' + algorithm + \"/\" + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "        print('Evaluation mode: result saved in evaluation folder')\n",
    "    else:\n",
    "        with open('results/' + prompt_type + '/' + output_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'gpt-4-0125-preview'\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=MODELNAME,\n",
    "    api_key=open('api.txt', 'r').read(),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "INITPROMPT = 'initPrompt_V2.1.0.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfSummaryLength(prompt_type, summary_prompt, format_prompt, ticket_uri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/summary/' + summary_prompt).format(min=minChars, max=maxChars, ticket=ticket)\n",
    "    format_prompt = load_prompt('prompts/summary/' + format_prompt)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(prompt_type, output, 'summary', MODELNAME, INITPROMPT, summary_prompt, format_prompt, 'JsonOutputParser', ticket_uri, ticket, reruns, evalMode)\n",
    "\n",
    "\n",
    "async def runAnswerComparison(ticket, ticket_uri, consensus_temp, format_temp, answers, run, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    minChars = 39\n",
    "    maxChars = 70\n",
    "\n",
    "    consensus_prompt = load_prompt('prompts/summary/' + consensus_temp)\n",
    "    consensus_format_prompt = load_prompt('prompts/summary/' + format_temp)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = consensus_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"answer\": chain1}\n",
    "        | consensus_format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chain ###\n",
    "    output = await chain2.ainvoke({\"ticket\":ticket, \"role\":\"Software Engineer\", \"min\":minChars, \"max\":maxChars, \"answers\":answers})\n",
    "    \n",
    "    ### Save result ###\n",
    "    saveSelection('Consensus', output, 'summary', MODELNAME, consensus_temp, format_temp, 'JsonOutputParser', ticket_uri, ticket, run, evalMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPrompt = 'summaryLengthPrompt_V3.2.0.json'\n",
    "formatPrompt = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength('0Shot', summaryPrompt, formatPrompt, ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPrompt = 'summaryLengthPrompt_FewShot_V2.3.0.json'\n",
    "formatPrompt = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength('FewShot', summaryPrompt, formatPrompt, ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPrompt = 'summaryLengthPrompt_0ShotCoT_V2.1.0.json'\n",
    "formatPrompt = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength('0ShotCoT', summaryPrompt, formatPrompt, ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPrompt = 'summaryLengthPrompt_CoT_V1.2.0.json'\n",
    "formatPrompt = 'summaryLengthFormatPrompt_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/summary/summaryDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/summary/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfSummaryLength('FewShotCoT', summaryPrompt, formatPrompt, ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensusPrompt = 'summaryLengthConsensusPrompt_V1.0.0.json'\n",
    "consensusFormatPrompt = 'summaryLengthConsensusFormatPrompt_V1.2.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing dataset for consensus task ###\n",
    "consensusDataset = pd.read_csv('./evaluation/summary/gpt-4-0125-preview/totalSmells_1Run.csv')\n",
    "\n",
    "reduced_df = consensusDataset[['jira', 'ticketId', 'ticket_uri', 'reruns']].drop_duplicates().sort_values(by=['jira'])\n",
    "\n",
    "### Set variables ###\n",
    "amount = reduced_df.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0\n",
    "run = 0\n",
    "\n",
    "### Run consensus task ###\n",
    "for index, row in reduced_df.iterrows():\n",
    "    ticketUri = row['ticket_uri']\n",
    "    run = row['reruns']\n",
    "    sample = consensusDataset[((consensusDataset['jira'] == row['jira']) & (consensusDataset['ticketId'] == row['ticketId']))]\n",
    "    answers = prepareConsensusTask(sample)\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runAnswerComparison(ticket, ticketUri, consensusPrompt, consensusFormatPrompt, answers, run, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug Report Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatPrompt = 'bugReportStructureFormatPrompt_V2.2.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runBugReportDescriptionStructuring(prompt_type, bugreport_prompt, formatter_prompt, ticket_uri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/bugReportStructure/' + bugreport_prompt).format(bug_report=ticket)\n",
    "    formatter = load_prompt('prompts/bugReportStructure/' + formatter_prompt)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_bug_report\": chain1}\n",
    "        | formatter\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(prompt_type, output, 'bugreportStructure', MODELNAME, INITPROMPT, bugreport_prompt, formatter_prompt, 'JsonOutputParser', ticket_uri, ticket, reruns, evalMode)\n",
    "\n",
    "async def runFewShotBugReportDescriptionStructuring(prompt_type, bugreport_prompt, formatter_prompt, example_prompt, ticket_uri, ticket, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    examples_prompt = load_prompt('prompts/bugReportStructure/' + example_prompt).format(bug_report=ticket)\n",
    "    bestPractice_prompt = load_prompt('prompts/bugReportStructure/' + bugreport_prompt).format(examples=examples_prompt)\n",
    "    formatter = load_prompt('prompts/bugReportStructure/' + formatter_prompt)\n",
    "\n",
    "    prompts = bugreport_prompt + \" ; \" + example_prompt\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_bug_report\": chain1}\n",
    "        | formatter\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(prompt_type, output, 'bugreportStructure', MODELNAME, INITPROMPT, prompts, formatter_prompt, 'JsonOutputParser', ticket_uri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportPrompt = 'bugReportStructurePrompt_V2.1.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/bugreportStructure/bugreportStructureDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/bugreportStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runBugReportDescriptionStructuring('0Shot', bugReportPrompt, formatPrompt, ticketUri, ticket, 1, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportPrompt = 'bugReportStructurePrompt_0ShotCoT_V1.0.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/bugreportStructure/bugreportStructureDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/bugreportStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runBugReportDescriptionStructuring('0ShotCoT', bugReportPrompt, formatPrompt, ticketUri, ticket, 1, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportPrompt = 'bugReportStructurePrompt_FewShot_V2.9.0.json'\n",
    "examplePrompt = 'bugReportStructurePrompt_FewShot_Examples_V1.2.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/bugreportStructure/bugreportStructureDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/bugreportStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runFewShotBugReportDescriptionStructuring('FewShot', bugReportPrompt, formatPrompt, examplePrompt, ticketUri, ticket, 1, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugReportPrompt = 'bugReportStructurePrompt_FewShotCoT_V1.6.0.json'\n",
    "examplePrompt = 'bugReportStructurePrompt_FewShotCoT_Examples_V1.5.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/bugreportStructure/bugreportStructureDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "errorCounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## (1/0/42) Running ticket: ./data/bugreportStructure/Apache_12581156_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (2/0/42) Running ticket: ./data/bugreportStructure/Apache_12582724_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (3/0/42) Running ticket: ./data/bugreportStructure/Apache_12921688_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (4/0/42) Running ticket: ./data/bugreportStructure/Hyperledger_34318_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (5/0/42) Running ticket: ./data/bugreportStructure/Hyperledger_42808_1.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (6/0/42) Running ticket: ./data/bugreportStructure/Hyperledger_43412_3.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (7/0/42) Running ticket: ./data/bugreportStructure/IntelDAOS_18269_1.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (8/0/42) Running ticket: ./data/bugreportStructure/IntelDAOS_21222_1.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (9/0/42) Running ticket: ./data/bugreportStructure/IntelDAOS_34753_2.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (10/0/42) Running ticket: ./data/bugreportStructure/JFrog_33330_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (11/0/42) Running ticket: ./data/bugreportStructure/JFrog_50425_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (12/0/42) Running ticket: ./data/bugreportStructure/JFrog_67061_12.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (13/0/42) Running ticket: ./data/bugreportStructure/Jira_293028_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (14/0/42) Running ticket: ./data/bugreportStructure/Jira_1075133_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (15/0/42) Running ticket: ./data/bugreportStructure/Jira_1388024_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (16/0/42) Running ticket: ./data/bugreportStructure/JiraEcosystem_47609_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (17/0/42) Running ticket: ./data/bugreportStructure/JiraEcosystem_167207_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (18/0/42) Running ticket: ./data/bugreportStructure/JiraEcosystem_180305_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (19/0/42) Running ticket: ./data/bugreportStructure/Mojang_72145_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (20/0/42) Running ticket: ./data/bugreportStructure/Mojang_232262_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (21/0/42) Running ticket: ./data/bugreportStructure/Mojang_262457_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (22/0/42) Running ticket: ./data/bugreportStructure/MongoDB_11140_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (23/0/42) Running ticket: ./data/bugreportStructure/MongoDB_1855974_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (24/0/42) Running ticket: ./data/bugreportStructure/MongoDB_1927891_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (25/0/42) Running ticket: ./data/bugreportStructure/Qt_164709_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (26/0/42) Running ticket: ./data/bugreportStructure/Qt_172449_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (27/0/42) Running ticket: ./data/bugreportStructure/Qt_180531_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (28/0/42) Running ticket: ./data/bugreportStructure/RedHat_12502428_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (29/0/42) Running ticket: ./data/bugreportStructure/RedHat_12570539_1.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (30/0/42) Running ticket: ./data/bugreportStructure/RedHat_12622649_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (31/0/42) Running ticket: ./data/bugreportStructure/Sakai_31076_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (32/0/42) Running ticket: ./data/bugreportStructure/Sakai_41985_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (33/0/42) Running ticket: ./data/bugreportStructure/Sakai_53927_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (34/0/42) Running ticket: ./data/bugreportStructure/SecondLife_66847_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (35/0/42) Running ticket: ./data/bugreportStructure/SecondLife_78655_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (36/0/42) Running ticket: ./data/bugreportStructure/SecondLife_141605_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (37/0/42) Running ticket: ./data/bugreportStructure/Sonatype_431036_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (38/0/42) Running ticket: ./data/bugreportStructure/Sonatype_722927_2.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (39/0/42) Running ticket: ./data/bugreportStructure/Sonatype_779079_3.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (40/0/42) Running ticket: ./data/bugreportStructure/Spring_16415_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (41/0/42) Running ticket: ./data/bugreportStructure/Spring_48981_2.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n",
      "######## (42/0/42) Running ticket: ./data/bugreportStructure/Spring_73274_0.json ########\n",
      "     \n",
      "The annotation was created successfully!\n",
      "Evaluation mode: result saved in evaluation folder\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/bugreportStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(errorCounter) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runFewShotBugReportDescriptionStructuring('FewShotCoT', bugReportPrompt, formatPrompt, examplePrompt, ticketUri, ticket, 0, True)\n",
    "        print(\"     \")\n",
    "    except:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        errorCounter += 1\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbitrary Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDetectionOfArbitraryStructure(init_prompt_name, bestPractice_prompt_name, format_prompt_name, ticketUri, ticket, issue_type, desired_structure, structure_desc, reruns= 0, evalMode=False):\n",
    "    ### Set Prompts ### \n",
    "    init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "    bestPractice_prompt = load_prompt('prompts/arbitraryStructure/'+bestPractice_prompt_name).format(issue_type=issue_type, structure_desc=structure_desc, structure=desired_structure)\n",
    "    format_prompt = load_prompt('prompts/arbitraryStructure/'+format_prompt_name)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = chain2.invoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    ### Save result ###\n",
    "    save_result(output, 'summary', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_type = \"User Story\"\n",
    "desired_structure = \"As a <Role>, I want/want to/need/can/would <Task>, so that <Goal>\"\n",
    "structure_desc = \"\"\"Role: abstract behavior of actors in the system context; describes who uses the system.\n",
    "Task: specific things that must be done to achieve goals; solution or function of the problem.\n",
    "Goal: a condition or a circumstance desired by stakeholders or actors; describes the problem domain or the impact of solving the problem.\"\"\"\n",
    "# structure_desc = \"\"\"Role: abstract behavior of actors in the system context; describes who uses the system.\n",
    "# Task: specific things that must be done to achieve goals; solution or function of the problem.\n",
    "# Goal: a condition or a circumstance desired by stakeholders or actors; describes the problem domain or the impact of solving the problem.\n",
    "# Capability: the ability of actors to achieve goals based on certain conditions and events.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/arbitraryStructure/arbitraryStructureDataset.csv')\n",
    "\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/arbitraryStructure/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "\n",
    "    with open(ticketUri) as f:\n",
    "        json_sample = json.load(f)\n",
    "    \n",
    "    try:\n",
    "        runDetectionOfArbitraryStructure('initPrompt_V1.3.0.json', 'arbitraryStructurePrompt_V1.0.0.json', 'arbitraryStructureFormatPrompt_V1.0.0.json', ticketUri, json_sample, issue_type, desired_structure, structure_desc)\n",
    "    except:\n",
    "        print(\"Error with ticket: \" + ticketUri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATEPROMPT = 'updatePrompt_V1.26.0.json'\n",
    "UPDATEFORMATPROMPT = 'updateFormatPrompt_V1.11.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runDetectionOfOutdatedFields(ticket, ticketUri, reruns= 0, evalMode=False):\n",
    "    ### Set variables ###\n",
    "    init_prompt = load_prompt('prompts/init/' + INITPROMPT)\n",
    "    bestPractice_prompt = load_prompt('prompts/update/' + UPDATEPROMPT).format()\n",
    "    format_prompt = load_prompt('prompts/update/' + UPDATEFORMATPROMPT)\n",
    "\n",
    "    ### Create multi chain ###\n",
    "    chain1 = init_prompt | model | StrOutputParser()\n",
    "    chain2 = (\n",
    "        {\"revised_ticket\": chain1}\n",
    "        | format_prompt\n",
    "        | model\n",
    "        | JsonOutputParser()\n",
    "    )\n",
    "\n",
    "    ### Run chains ###\n",
    "    output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":ticket})\n",
    "\n",
    "    # ### Save result ###\n",
    "    save_result(output, 'update', MODELNAME, INITPROMPT, UPDATEPROMPT, UPDATEFORMATPROMPT, 'JsonOutputParser', ticketUri, ticket, reruns, evalMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedDataset = pd.read_csv('data/update/updateDataset.csv')\n",
    "\n",
    "amount = annotatedDataset.shape[0]\n",
    "num = 1\n",
    "for index, row in annotatedDataset.iterrows():\n",
    "    ticketUri = \"./data/update/\" + row['Jira'] + \"_\" + str(row['IssueId']) + \"_\" + str(row['EvoId']) + \".json\"\n",
    "    \n",
    "    with open(ticketUri) as f:\n",
    "        ticket = json.load(f)\n",
    "    try:\n",
    "        print(\"######## (\" + str(num) + \"/\" + str(amount) + \") Running ticket: \" + ticketUri + \" ########\")\n",
    "        print(\"     \")\n",
    "        await runDetectionOfOutdatedFields(ticket, ticketUri, 1, True)\n",
    "        print(\"     \")\n",
    "    except Exception as e:\n",
    "        print('Error with ticket: ' + ticketUri)\n",
    "        print(e)\n",
    "        continue\n",
    "    finally:\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_toxic.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'toxicSpeechPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/toxicSpeech/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'toxicSpeechFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/toxicSpeech/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"revised_ticket\": chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Content Moderator\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'toxicSpeech', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"last_row_german.json\"\n",
    "with open('./data/json/last_row/'+sample_name) as f:\n",
    "    json_sample = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_name = 'initPrompt_V1.3.0.json'\n",
    "init_prompt = load_prompt('prompts/init/'+init_prompt_name)\n",
    "\n",
    "bestPractice_prompt_name = 'internationalizationPrompt_V1.5.0.json'\n",
    "bestPractice_prompt = load_prompt('prompts/internationalization/'+bestPractice_prompt_name).format()\n",
    "\n",
    "format_prompt_name = 'internationalizationFormatPrompt_V1.2.0.json'\n",
    "format_prompt = load_prompt('prompts/internationalization/'+format_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = init_prompt | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"revised_ticket\":chain1}\n",
    "    | format_prompt\n",
    "    | model\n",
    "    | JsonOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await chain2.ainvoke({\"role\":\"Software Engineer\", \"best_practice\":bestPractice_prompt, \"ticket\":json_sample})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(output, 'internationalization', MODELNAME, init_prompt_name, bestPractice_prompt_name, format_prompt_name, 'JsonOutputParser', sample_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
